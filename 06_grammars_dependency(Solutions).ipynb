{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dependency Grammars with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding:\n",
    "    - Dependency Relations and Grammars\n",
    "    - Probabilistic Dependency Grammars\n",
    "    - Projective and Non-Projective Parses\n",
    "    - Transition-based Dependency Parsing\n",
    "\n",
    "- Learning how to:\n",
    "    - define dependency grammar in NLTK\n",
    "    - identify a syntactic relation between Head and Dependent\n",
    "    - parse with dependency grammar\n",
    "    - evaluate dependency parser\n",
    "    - use dependency parser of spacy and stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 18: Dependency Parsing](https://web.stanford.edu/~jurafsky/slp3/18.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 8: Analyzing Sentence Structure](https://www.nltk.org/book/ch08.html)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English models\n",
    "- [stanza](https://stanfordnlp.github.io/stanza/) for Stanford Parser\n",
    "    - run `pip install stanza`\n",
    "    - run `stanza.download('en')` to intall English models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Dependency Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Constituency (Phrase Structure) Grammar that addresses how words and sequences of words combine to form constituents, Dependency Grammar addresses on how words relate to each other. \n",
    "\n",
    "Dependency Grammar assumes that syntactic structure consists of words linked by binary, asymmetrical relations called __dependency relations__. A dependency relation is a binary asymmetric relation that holds between a syntactically subordinate word, called the __dependent__, and another word on which it depends, called the __head__.\n",
    "\n",
    "The __head of a sentence__ is usually taken to be the tensed verb, and every other word is either dependent on the sentence head, or connects to it through a path of dependencies. Thus, a dependency parse is a __directed graph__, where the nodes are the lexical items (words) and the arcs represent dependency relations from heads to dependents. \n",
    "\n",
    "A __typed dependency structure__ contains of the __labeled__ arcs are drawn from a fixed inventory of grammatical relations, that also includes a __root node__ that explicitly marks the root of the tree, the head of the entire structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Dependency Relation Types\n",
    "\n",
    "Universal Dependency set defines the following __core__ relations (from Jurafsky & Martin). \n",
    "\n",
    "(See https://universaldependencies.org/u/dep/index.html for the full set.)\n",
    "\n",
    "| __Clausal Argument Relations__ | Description | Example |\n",
    "|:-------------------------------|:------------|:--------\n",
    "| NSUBJ  | Nominal subject       | __We__ booked her the cheapest morning flight to Miami.\n",
    "| DOBJ   | Direct object         | We booked her the cheapest morning __flight__ to Miami.\n",
    "| IOBJ   | Indirect object       | We booked __her__ the cheapest morning flight to Miami.\n",
    "| CCOMP  | Clausal complement    |\n",
    "| XCOMP  | Open clausal complement (subject of clause is out of its span) \n",
    "| __Nominal Modifier Relations__ ||\n",
    "| NMOD   | Nominal modifier      | We booked her the cheapest __morning__ flight to Miami.\n",
    "| AMOD   | Adjectival modifier   | We booked her the __cheapest__ morning flight to Miami.\n",
    "| NUMMOD | Numeric modifier\n",
    "| APPOS  | Appositional modifier\n",
    "| DET    | Determiner            | We booked her __the__ cheapest morning flight to Miami.\n",
    "| CASE   | Prepositions, postpositions and other case markers | We booked her the cheapest morning flight __to__ Miami.\n",
    "| __Other Notable Relations__ | \n",
    "| CONJ   | Conjunct\n",
    "| CC     | Coordinating conjunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Defining Dependency Grammar in NLTK\n",
    "\n",
    "Similar to Phrase Structure Grammar, Dependecy Grammar is defined as a list of production rules.\n",
    "\n",
    "Below is an example grammar that defines only __bare__ dependency relations without specifying their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "s_bold = '\\033[1m'\n",
    "e_bold = '\\033[0m'\n",
    "\n",
    "# for sentence \"i saw the man with the telescope\"\n",
    "# only string input is accepted\n",
    "rules = \"\"\"\n",
    "    'saw' -> 'i' | 'man' | 'with'\n",
    "    'man' ->  'telescope' | 'the' | 'with'\n",
    "    'telescope' -> 'the' | 'with' | 'a'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "toy_grammar = nltk.DependencyGrammar.fromstring(rules)\n",
    "\n",
    "print(toy_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Phrase Structure Grammar, \n",
    "\n",
    "- there is no start symbol (thus, no method to access it)\n",
    "- there is no method to access productions, but it is still possible using the attribute\n",
    "\n",
    "    - `grammar._productions`\n",
    "\n",
    "- there is a method to check if grammar contains a production\n",
    "\n",
    "    - `grammar.contains(head, mod)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Dependency Production__ has 2 attributes:\n",
    "\n",
    "- `_lhs` (left-hand side) -- head\n",
    "- `_rhs` (right-hand side) -- modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(toy_grammar._productions)\n",
    "\n",
    "for production in toy_grammar._productions:\n",
    "    print(production._lhs, production._rhs)\n",
    "\n",
    "print(toy_grammar.contains('man', 'the'))  # True\n",
    "print(toy_grammar.contains('the', 'man'))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to Identify a Syntactic Relation between Head and Dependent\n",
    "(From Kübler et al. & NLTK Book)\n",
    "\n",
    "Here is a list of some of the more common criteria that have been proposed for identifying a syntactic relation between a head __H__ and a dependent __D__ in a linguistic construction __C__:\n",
    "\n",
    "1. __H__ determines the syntactic category of __C__ and can often replace __C__.\n",
    "2. __H__ determines the semantic category of __C__; __D__ gives semantic specification.\n",
    "3. __H__ is obligatory; __D__ may be optional.\n",
    "4. __H__ selects __D__ and determines whether __D__ is obligatory or optional. \n",
    "5. The form of __D__ depends on __H__ (agreement or government).\n",
    "6. The linear position of __D__ is specified with reference to __H__ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__:\n",
    "\n",
    "_I prefer a morning flight_\n",
    "\n",
    "- **C**: _morning flight_\n",
    "- **H**: _flight_\n",
    "    - determines syntactic category: whole construction is nominal\n",
    "    - determines semantic category\n",
    "    - comes after _morning_ (English is head final)\n",
    "- **D**: _morning_\n",
    "    - optional w.r.t. _flight_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Parsing with Dependency Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since Dependency Graphs can be projective and non-projective (allow crossing dependencies), there are __projective__ and __non-projective__ parsers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.1. Projective Dependency Parser (Rule-based)\n",
    ">**Definition**:\n",
    "A dependecy tree is projective if **all the arcs of the tree are projective**. An arc from a head to a dependent is said to be projective projective <mark style=\"background-color: rgba(0, 255, 0, 0.2)\"> if there is a path from the head to every word that lies between the head and the dependent </mark> in the sentence. *(Dan Jurafsky and James H. Martin, 2022)*\n",
    "\n",
    "> **NLTK**: A projective, rule-based, dependency parser. A [`ProjectiveDependencyParser`](http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.projectivedependencyparser) is created with a `DependencyGrammar`, a set of productions specifying word-to-word dependency relations. The `parse()` method will then return the set of all parses, in tree representation, for a given input sequence of tokens. \n",
    "`parse()` method returns iterator over [`Tree`](http://www.nltk.org/_modules/nltk/tree.html) objects.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parser = nltk.ProjectiveDependencyParser(toy_grammar)\n",
    "\n",
    "sent = \"i saw the man with a telescope\"\n",
    "\n",
    "for tree in parser.parse(sent.split()):\n",
    "    print(tree.pretty_print(unicodelines=True, nodedist=4))\n",
    "    # print ROOT node\n",
    "    print(\"The ROOT is '{}'\".format(s_bold + tree.label() + e_bold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.2. Non-Projective Dependency Parser (Rule-Based)\n",
    "\n",
    ">**Definition**:\n",
    "A dependecy tree is projective if **one or more arcs of the tree are non-projective**. An arc non-projective  when there some words that lies between the head and the dependent in the sentence do not have a path from the head.\n",
    "\n",
    "\n",
    "> A non-projective, rule-based, dependency parser. This parser will return the set of all possible non-projective parses based on the word-to-word relations defined in the parser’s dependency grammar, and <mark style=\"background-color: rgba(0, 255, 0, 0.2)\"> will allow the branches of the parse tree to cross</mark> in order to capture a variety of linguistic phenomena that a projective parser will not .\n",
    "\n",
    "`parse()` method returns iterator over [`DependencyGraph`](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) objects. \n",
    "\n",
    "`tree()` method of the `DependencyGraph` object builds a dependency tree using the NLTK Tree constructor, starting with the `root` node and omitting labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.postimg.cc/hvQDRNDg/Screenshot-2022-12-19-at-17-22-53.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flight -> was **is not projective**, since *this morning* is not reachable from *flight*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np_parser = nltk.NonprojectiveDependencyParser(toy_grammar)\n",
    "\n",
    "for graph in np_parser.parse(sent.split()):\n",
    "    graph.tree().pretty_print(unicodelines=True, nodedist=4)\n",
    "    print(\"The ROOT is '{}'\".format(s_bold + graph.root['word'] + e_bold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the sentence is ambiguous, similar to Phrase Structure Grammar, our Dependency Grammar yields 2 parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.3. Accessing the Graph\n",
    "\n",
    "- `DependencyGraph` object has 2 attrubutes\n",
    "    - nodes (of `defaultdict` type)\n",
    "    - root (of `dict` type), which is also a node\n",
    "\n",
    "- Each node in a graph is represented as a dict that defines its:\n",
    "    - address (sentence index starting from 1) -- required\n",
    "    - word (string form) -- required\n",
    "    - head (address)\n",
    "    - deps (dependents)\n",
    "    - rel (dependency relation to head)\n",
    "\n",
    "Thus, we can print the graph as a list of tokens with their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# printing root address and word\n",
    "for graph in np_parser.parse(sent.split()): \n",
    "    print(graph.root['address'], graph.root['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# printing all the nodes with dependent positions\n",
    "for graph in np_parser.parse(sent.split()):    \n",
    "    # sorting is required since graph starts from root, which is not the first token\n",
    "    for _, node in sorted(graph.nodes.items()):\n",
    "        if node['word'] is not None:\n",
    "            print('{address}\\t{word}:\\t{dependents}'.format(dependents=node['deps'][''], **node))\n",
    "    break  # just to print 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to convert the graph into other supported formats, such as CoNLL using `to_conll(style)` method, where [style](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) is either 3, 4, or 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for graph in np_parser.parse(sent.split()):    \n",
    "    print(graph.to_conll(3))\n",
    "    break  # just to print 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Define grammar that covers the following sentences.\n",
    "\n",
    "    - show flights from new york to los angeles\n",
    "    - list flights from new york to los angeles\n",
    "    - show flights from new york\n",
    "    - list flights to los angeles\n",
    "    - list flights\n",
    "    \n",
    "- Use one of the parsers to parse the sentences (i.e. test your grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# As reference  \n",
    "# rules = \"\"\"\n",
    "#     'saw' -> 'i' | 'man' | 'with'\n",
    "#     'man' ->  'telescope' | 'the' | 'with'\n",
    "#     'telescope' -> 'the' | 'with' | 'a'\n",
    "# \"\"\"\n",
    "\n",
    "sentences = ['show flights from new york to los angeles', \n",
    "             'show flights from los angeles to new york'\n",
    "             'list flights from new york to los angeles',\n",
    "            'show flights from new york', \n",
    "             'list flights to los angeles', \n",
    "             'list flights']\n",
    "rules = \"\"\"\n",
    "    'show' -> 'flights' \n",
    "    'list' -> 'flights'\n",
    "    'flights'  -> 'york' | 'angeles'\n",
    "    'york' -> 'new' | 'from' | 'to'\n",
    "    'angeles' -> 'los' | 'from' | 'to'\n",
    "\"\"\"\n",
    "\n",
    "toy_grammar = nltk.DependencyGrammar.fromstring(rules)\n",
    "\n",
    "np_parser = nltk.ProjectiveDependencyParser(toy_grammar)\n",
    "for sent in sentences:\n",
    "    for graph in np_parser.parse(sent.split()):\n",
    "        print(\"Sentence:\", sent)\n",
    "\n",
    "        if type(graph) != nltk.tree.Tree:\n",
    "            graph.tree().pretty_print(unicodelines=True, nodedist=4)\n",
    "            print(\"The ROOT is '{}'\".format(s_bold + graph.root['word'] + e_bold), '\\n')\n",
    "        else:\n",
    "            graph.pretty_print(unicodelines=True, nodedist=4)\n",
    "            print(\"The ROOT is '{}'\".format(s_bold + graph.label() + e_bold), '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Probabilistic Dependency Grammars & Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similar to CFGs, we can learn dependency grammar from data using treebanks.\n",
    "NLTK provides `ProbabilisticProjectiveDependencyParser` that returns the most probable projective parse derived from the probabilistic dependency grammar derived from the `train()` method. \n",
    "\n",
    "> The probabilistic model is an implementation of Eisner's (1996) Model C, which conditions on head-word, head-tag, child-word, and child-tag. The decoding uses a bottom-up chart-based span concatenation algorithm that's identical to the one utilized by the rule-based projective parser.\n",
    "\n",
    "Without going into details, that is an example of Dynamic Programming approach to Dependency Parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# downloading treebank\n",
    "import nltk\n",
    "nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# example from NLTK\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from nltk.parse import ProbabilisticProjectiveDependencyParser\n",
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "# print dependency graph in CoNLL format\n",
    "print(dependency_treebank.parsed_sents()[0].to_conll(10))\n",
    "\n",
    "ppdp = ProbabilisticProjectiveDependencyParser()\n",
    "\n",
    "# train parser on graphs\n",
    "ppdp.train(dependency_treebank.parsed_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# parse the sentence\n",
    "parse = ppdp.parse(['the', 'price', 'of', 'the', 'stock', 'fell'])\n",
    "\n",
    "# returns set of trees ordered by probability score\n",
    "for tree in parse:\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function that given a dependency graph, for each token (word), produces list of words from it to ROOT.\n",
    "\n",
    "(Construct normal `dict` for simplicity first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# With .nodes we get a dict\n",
    "dg_tree = dependency_treebank.parsed_sents()[0].tree()\n",
    "dg = dependency_treebank.parsed_sents()[0].nodes\n",
    "# Let's print to see what it contains\n",
    "#pprint(dg)\n",
    "print(dg_tree.pretty_print(unicodelines=True, nodedist=4))\n",
    "\n",
    "def go_to_root(token, head, dg):\n",
    "    end = False\n",
    "    path = [token]\n",
    "    if head == 0:\n",
    "        return path\n",
    "    while not end:\n",
    "        next_token = dg[head]['word']\n",
    "        head = dg[head]['head']\n",
    "        path.append(next_token)\n",
    "        if head == 0:\n",
    "            end = True\n",
    "    return path\n",
    "\n",
    "for k, v in sorted(dg.items()):\n",
    "    if k != 0:\n",
    "        print(v['word'],\":\" ,go_to_root(v['word'], v['head'], dg))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are several methods for data-driven dependency parsing\n",
    "- Dynamic Programming-based (e.g. `ProbabilisticProjectiveDependencyParser`)\n",
    "- Graph-based (e.g. [Minimum Spanning Tree Parser](https://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html))\n",
    "- Transition-Based Dependency Parsing (e.g. NLTK's interface to [MaltParser](https://www.nltk.org/_modules/nltk/parse/malt.html))\n",
    "\n",
    "Transition-based parsing (or \"deterministic dependency parsing\") proved to be very effective and is the State-of-the-Art approach (with neural twist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from Jurafsky & Martin)\n",
    "\n",
    "In transition-based parsing there is:\n",
    "- a **stack** on which we build the parse\n",
    "- a **buffer** of tokens to be parsed\n",
    "- a **parser** which takes actions on the parse via a predictor called an **oracle**\n",
    "\n",
    "The parser walks through the sentence left-to-right, successively shifting items from the buffer onto the stack. At each time point we examine the top two elements on the stack, and the oracle makes a decision about what transition to apply to build the parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arc-Standard Transition System (there are alternatives, i.e. Arc Eager) defines 3 transition operators that will operate on the top two elements of the stack:\n",
    "\n",
    "- __LEFTARC__: \n",
    "    - Assert a head-dependent relation between the word at the top of the stack and the word directly beneath it;\n",
    "    - Remove the lower word from the stack.\n",
    "- __RIGHTARC__: \n",
    "    - Assert a head-dependent relation between the second word on the stack and the word at the top; \n",
    "    - Remove the word at the top of the stack;\n",
    "- __SHIFT__: \n",
    "    - Remove the word from the front of the input buffer and push it onto the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally fits into Machine Learning framework where \n",
    "- configuration (buffer & stack) are features, \n",
    "- operations are labels\n",
    "- oracle is a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transition Parser in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.parse.transitionparser import TransitionParser\n",
    "\n",
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(dependency_treebank.parsed_sents()[:100], 'tp.model')\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-10:], 'tp.model')\n",
    "print(len(parses))\n",
    "print(parses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Evaluation of Dependency Parsing\n",
    "\n",
    "Dependency Parsing performance is evaluated as __labeled__ and __unlabeled attachment scores__ which are calculated as \n",
    "\n",
    "$$ UAS/LAS = \\frac{\\text{# of corrent dependency relations}}{\\text{# of dependency relations}}$$\n",
    "\n",
    "the difference between the two is whether the relation labels are considered or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLTK provides `DependencyEvaluator` class to perform the evaluation, that takes predicted and reference parses as arguments. The evaluation ignores punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-10:])\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(las)\n",
    "print(uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Train `arc-standard` and `arc-eager` transition parsers on the same portion (slightly bigger than 100, otherwise it takes a lot of time)\n",
    "- Evaluate both of them comparing the attachment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(dependency_treebank.parsed_sents()[:100], 'tp.model', verbose=False)\n",
    "\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-150:], 'tp.model')\n",
    "print(dependency_treebank.parsed_sents()[-1:])\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-150:])\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(las)\n",
    "print(uas)\n",
    "\n",
    "tp = TransitionParser('arc-eager')\n",
    "tp.train(dependency_treebank.parsed_sents()[:100], 'tp.model', verbose=False)\n",
    "\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-150:], 'tp.model')\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-150:])\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(las)\n",
    "print(uas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parses = tp.parse(dependency_treebank.parsed_sents()[-1:], 'tp.model')\n",
    "parses[0].tree().pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Dependency Parsing with Stanza and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both spaCy and Stanza (python package Stanford NLP Tools) provide pre-trained dependency parsing models.\n",
    "The libraries are quite similar in usage.\n",
    "\n",
    "- initialize pipeline (with other processing steps such as tokenization, POS-tagging)\n",
    "- process a sentence \n",
    "- iterate over tokens accessing dependency parsing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Stanford Dependency Parser\n",
    "\n",
    "[paper on stanza](https://arxiv.org/pdf/2003.07082.pdf)\n",
    "\n",
    "A neural graph-based dependency parser. \n",
    "[paper on parser](https://nlp.stanford.edu/pubs/dozat2017deep.pdf)\n",
    "\n",
    "> We implement a Bi-LSTM-based deep biaffine neural dependency parser (Dozat and Manning, 2017). We\n",
    "further augment this model with two linguistically motivated features: one that predicts the linearization order of two words in a given language, and the other that predicts the typical distance in linear order between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2. Spacy Dependency Parser\n",
    "\n",
    "> A transition-based dependency parser component. The dependency parser jointly learns sentence segmentation and labelled dependency parsing, and can optionally learn to merge tokens that had been over-segmented by the tokenizer. The parser uses a variant of the non-monotonic arc-eager transition-system described by Honnibal and Johnson (2014), with the addition of a \"break\" transition to perform the sentence segmentation. Nivre (2005)’s pseudo-projective dependency transformation is used to allow the parser to predict non-projective parses.\n",
    "\n",
    "> The parser is trained using an imitation learning objective. It follows the actions predicted by the current weights, and at each state, determines which actions are compatible with the optimal parse that could be reached from the current state. The weights are updated such that the scores assigned to the set of optimal actions is increased, while scores assigned to other actions are decreased. Note that more than one action may be optimal for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "example = 'I saw the man with a telescope.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# stanza example\n",
    "import stanza\n",
    "\n",
    "# Download the stanza model if necessary\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "stanza_nlp = stanza.Pipeline(lang='en', verbose=False)\n",
    "stanza_doc = stanza_nlp(example)\n",
    "\n",
    "for sent in stanza_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(word.id, word.text, word.head, word.deprel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# spacy example\n",
    "import spacy\n",
    "\n",
    "# spacy_nlp = spacy.load(\"en-core-web-sm\")\n",
    "\n",
    "# un-comment the lines below, if you get 'ModuleNotFoundError'\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "spacy_doc = spacy_nlp(example)\n",
    "\n",
    "for sent in spacy_doc.sents:\n",
    "    for token in sent:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(token.i, token.text, token.head, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse 100 last sentences from dependency treebank using `spacy` and `stanza`\n",
    "    - are the depedency tags of spacy the same of stanza?\n",
    "- Evaluate against the ground truth the parses using DependencyEvaluator\n",
    "    - print LAS and UAS for each parser\n",
    "\n",
    "**BUT!** To evaluate the parsers, the sentences parsed by spacy and stanza have to be [`DependencyGraph`](https://www.nltk.org/_modules/nltk/parse/dependencygraph.html) objects.  To do this , you have to covert the output of the spacy/stanza to [ConLL](https://universaldependencies.org/format.html) formant, from this format extract the columns following the [Malt-Tab](https://cl.lingfil.uu.se/~nivre/research/MaltXML.html) format and finally convert the resulting string into a DependecyGraph. Lucky for you there is a library that gets the job done.  You have to install the library [spacy_conll](https://github.com/BramVanroy/spacy_conll) and use and adapt to your needs the code that you can find below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Army', 'Corps', 'is', 'cutting', 'the', 'flow', 'of', 'the', 'Missouri', 'River', 'about', 'two', 'weeks', 'earlier', 'than', 'normal', 'because', 'of', 'low', 'water', 'levels', 'in', 'the', 'reservoirs', 'that', 'feed', 'it', '.'], ['Barge', 'rates', 'on', 'the', 'Mississippi', 'River', 'sank', 'yesterday', 'on', 'speculation', 'that', 'widespread', 'rain', 'this', 'week', 'in', 'the', 'Midwest', 'might', 'temporarily', 'alleviate', 'the', 'situation', '.'], ...]\n",
      "[<DependencyGraph with 30 nodes>, <DependencyGraph with 25 nodes>, <DependencyGraph with 17 nodes>, <DependencyGraph with 42 nodes>, <DependencyGraph with 19 nodes>, <DependencyGraph with 30 nodes>, <DependencyGraph with 28 nodes>, <DependencyGraph with 29 nodes>, <DependencyGraph with 29 nodes>, <DependencyGraph with 34 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 34 nodes>, <DependencyGraph with 15 nodes>, <DependencyGraph with 38 nodes>, <DependencyGraph with 29 nodes>, <DependencyGraph with 30 nodes>, <DependencyGraph with 39 nodes>, <DependencyGraph with 17 nodes>, <DependencyGraph with 19 nodes>, <DependencyGraph with 18 nodes>, <DependencyGraph with 15 nodes>, <DependencyGraph with 14 nodes>, <DependencyGraph with 20 nodes>, <DependencyGraph with 7 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 25 nodes>, <DependencyGraph with 23 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 10 nodes>, <DependencyGraph with 18 nodes>, <DependencyGraph with 13 nodes>, <DependencyGraph with 12 nodes>, <DependencyGraph with 13 nodes>, <DependencyGraph with 43 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 17 nodes>, <DependencyGraph with 13 nodes>, <DependencyGraph with 37 nodes>, <DependencyGraph with 24 nodes>, <DependencyGraph with 18 nodes>, <DependencyGraph with 34 nodes>, <DependencyGraph with 46 nodes>, <DependencyGraph with 24 nodes>, <DependencyGraph with 8 nodes>, <DependencyGraph with 28 nodes>, <DependencyGraph with 27 nodes>, <DependencyGraph with 35 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 27 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 27 nodes>, <DependencyGraph with 21 nodes>, <DependencyGraph with 14 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 28 nodes>, <DependencyGraph with 33 nodes>, <DependencyGraph with 29 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 39 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 34 nodes>, <DependencyGraph with 46 nodes>, <DependencyGraph with 42 nodes>, <DependencyGraph with 26 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 21 nodes>, <DependencyGraph with 24 nodes>, <DependencyGraph with 26 nodes>, <DependencyGraph with 19 nodes>, <DependencyGraph with 17 nodes>, <DependencyGraph with 35 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 33 nodes>, <DependencyGraph with 21 nodes>, <DependencyGraph with 31 nodes>, <DependencyGraph with 14 nodes>, <DependencyGraph with 30 nodes>, <DependencyGraph with 43 nodes>, <DependencyGraph with 51 nodes>, <DependencyGraph with 20 nodes>, <DependencyGraph with 52 nodes>, <DependencyGraph with 29 nodes>, <DependencyGraph with 16 nodes>, <DependencyGraph with 32 nodes>, <DependencyGraph with 41 nodes>, <DependencyGraph with 24 nodes>, <DependencyGraph with 19 nodes>, <DependencyGraph with 28 nodes>, <DependencyGraph with 41 nodes>, <DependencyGraph with 43 nodes>, <DependencyGraph with 22 nodes>, <DependencyGraph with 6 nodes>, <DependencyGraph with 16 nodes>]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "# get the last 100 sentences\n",
    "data = dependency_treebank.sents()[-100:]\n",
    "tagged_data = dependency_treebank.parsed_sents()[-100:]\n",
    "print(data)\n",
    "print(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'conll_formatter' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m config \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mext_names\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mconll_pd\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     11\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mconversion_maps\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mDEPREL\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mnsubj\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msubj\u001b[39m\u001b[39m\"\u001b[39m}}}\n\u001b[1;32m     13\u001b[0m \u001b[39m# Add the formatter to the pipeline\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(\u001b[39m\"\u001b[39;49m\u001b[39mconll_formatter\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Split by white space\u001b[39;00m\n\u001b[1;32m     16\u001b[0m nlp\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m Tokenizer(nlp\u001b[39m.\u001b[39mvocab)  \n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/spacy/language.py:786\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    782\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[1;32m    783\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[1;32m    784\u001b[0m     )\n\u001b[1;32m    785\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 786\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[1;32m    787\u001b[0m         factory_name,\n\u001b[1;32m    788\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    789\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    790\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[1;32m    791\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    792\u001b[0m     )\n\u001b[1;32m    793\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[1;32m    794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/spacy/language.py:660\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[1;32m    653\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    654\u001b[0m         name\u001b[39m=\u001b[39mfactory_name,\n\u001b[1;32m    655\u001b[0m         opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m         lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[1;32m    659\u001b[0m     )\n\u001b[0;32m--> 660\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    661\u001b[0m pipe_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n\u001b[1;32m    662\u001b[0m \u001b[39m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'conll_formatter' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "# Spacy version \n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy \n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set up the conll formatter \n",
    "config = {\"ext_names\": {\"conll_pd\": \"pandas\"},\n",
    "          \"conversion_maps\": {\"DEPREL\": {\"nsubj\": \"subj\"}}}\n",
    "\n",
    "# Add the formatter to the pipeline\n",
    "nlp.add_pipe(\"conll_formatter\", config=config, last=True)\n",
    "# Split by white space\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  \n",
    "\n",
    "spacy_graphs = []\n",
    "\n",
    "for sentence in data:\n",
    "    # Join the words to a sentence\n",
    "    sentence = \" \".join(sentence)\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "    # Convert doc to a pandas object\n",
    "    df = doc._.pandas\n",
    "    # Select the columns accoroding to Malt-Tab format\n",
    "    tmp = df[[\"FORM\", 'XPOS', 'HEAD', 'DEPREL']].to_string(header=False, index=False)\n",
    "    print(tmp)\n",
    "    # Get finally our the DepencecyGraph\n",
    "    dp = DependencyGraph(tmp)\n",
    "    spacy_graphs.append(dp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Army', 'Corps', 'is', 'cutting', 'the', 'flow', 'of', 'the', 'Missouri', 'River', 'about', 'two', 'weeks', 'earlier', 'than', 'normal', 'because', 'of', 'low', 'water', 'levels', 'in', 'the', 'reservoirs', 'that', 'feed', 'it', '.']\n",
      "       The  DT  3      det\n",
      "      Army NNP  3 compound\n",
      "     Corps NNP  5    nsubj\n",
      "        is VBZ  5      aux\n",
      "   cutting VBG  0     ROOT\n",
      "       the  DT  7      det\n",
      "      flow  NN  5     dobj\n",
      "        of  IN  7     prep\n",
      "       the  DT 11      det\n",
      "  Missouri NNP 11 compound\n",
      "     River NNP  8     pobj\n",
      "     about  RB 13   advmod\n",
      "       two  CD 14   nummod\n",
      "     weeks NNS 15 npadvmod\n",
      "   earlier RBR  5   advmod\n",
      "      than  IN 15     prep\n",
      "    normal  JJ 16     amod\n",
      "   because  IN  5     prep\n",
      "        of  IN 18    pcomp\n",
      "       low  JJ 22     amod\n",
      "     water  NN 22 compound\n",
      "    levels NNS 18     pobj\n",
      "        in  IN 22     prep\n",
      "       the  DT 25      det\n",
      "reservoirs NNS 23     pobj\n",
      "      that WDT 27    nsubj\n",
      "      feed VBP 25    relcl\n",
      "        it PRP 27     dobj\n",
      "         .   .  5    punct\n"
     ]
    }
   ],
   "source": [
    "from spacy_conll import init_parser\n",
    "\n",
    "\n",
    "# Initialise English parser, already including the ConllFormatter as a pipeline component.\n",
    "# Indicate that we want to get the CoNLL headers in the string output.\n",
    "# `use_gpu` and `verbose` are specific to stanza. These keywords arguments are passed onto their Pipeline() initialisation\n",
    "nlp = init_parser(\"en_core_web_sm\",\n",
    "                  \"spacy\",\n",
    "                  include_headers=True)\n",
    "# Parse a given string\n",
    "print(data[0])\n",
    "doc = nlp(\" \".join(data[0]))\n",
    "\n",
    "# Get the CoNLL representation of the whole document, including headers\n",
    "conll = doc._.conll_pd\n",
    "tmp = conll[[\"FORM\", 'XPOS', 'HEAD', 'DEPREL']].to_string(header=False, index=False)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.6926873037236428\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "de = DependencyEvaluator(spacy_graphs, tagged_data)\n",
    "las, uas = de.eval()\n",
    "\n",
    "print(las)\n",
    "print(uas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "libcublas.so.*[0-9] not found in the system path ['/home/dhilab-mattia/Desktop/uni/nlu-labs', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/torch/__init__.py:168\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(lib_path, mode\u001b[39m=\u001b[39;49mctypes\u001b[39m.\u001b[39;49mRTLD_GLOBAL)\n\u001b[1;32m    169\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    170\u001b[0m     \u001b[39m# Can only happen for wheel with cuda libs as PYPI deps\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39m# As PyTorch is not purelib, but nvidia-*-cu11 is\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: libcurand.so.10: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Stanza\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanza\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy_stanza\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Download the stanza model if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/stanza/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m DownloadMethod, Pipeline\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmultilingual\u001b[39;00m \u001b[39mimport\u001b[39;00m MultilingualPipeline\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m Document\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/stanza/pipeline/core.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m langcode_to_lang\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m Document\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfoundation_cache\u001b[39;00m \u001b[39mimport\u001b[39;00m FoundationCache\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m default_device\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprocessor\u001b[39;00m \u001b[39mimport\u001b[39;00m Processor, ProcessorRequirementsException\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/stanza/models/common/foundation_cache.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m bert_embedding\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchar_model\u001b[39;00m \u001b[39mimport\u001b[39;00m CharacterLanguageModel\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrain\u001b[39;00m \u001b[39mimport\u001b[39;00m Pretrain\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/stanza/models/common/bert_embedding.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/torch/__init__.py:228\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 228\u001b[0m         _load_global_deps()\n\u001b[1;32m    229\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# torch._C module initialization code in C\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/torch/__init__.py:189\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    188\u001b[0m \u001b[39mfor\u001b[39;00m lib_folder, lib_name \u001b[39min\u001b[39;00m cuda_libs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 189\u001b[0m     _preload_cuda_deps(lib_folder, lib_name)\n\u001b[1;32m    190\u001b[0m ctypes\u001b[39m.\u001b[39mCDLL(lib_path, mode\u001b[39m=\u001b[39mctypes\u001b[39m.\u001b[39mRTLD_GLOBAL)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/torch/__init__.py:154\u001b[0m, in \u001b[0;36m_preload_cuda_deps\u001b[0;34m(lib_folder, lib_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lib_path:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlib_name\u001b[39m}\u001b[39;00m\u001b[39m not found in the system path \u001b[39m\u001b[39m{\u001b[39;00msys\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m ctypes\u001b[39m.\u001b[39mCDLL(lib_path)\n",
      "\u001b[0;31mValueError\u001b[0m: libcublas.so.*[0-9] not found in the system path ['/home/dhilab-mattia/Desktop/uni/nlu-labs', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages']"
     ]
    }
   ],
   "source": [
    "# Stanza\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "# Download the stanza model if necessary\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Set up the conll formatter \n",
    "#tokenize_pretokenized used to tokenize by white space \n",
    "nlp = spacy_stanza.load_pipeline(\"en\", verbose=False)\n",
    "\n",
    "config = {\"ext_names\": {\"conll_pd\": \"pandas\"},\n",
    "          \"conversion_maps\": {\"deprel\": {\"nsubj\": \"subj\", \"root\":\"ROOT\"}}}\n",
    "\n",
    "# Add the formatter to the pipeline\n",
    "nlp.add_pipe(\"conll_formatter\", config=config, last=True)\n",
    "\n",
    "# Parse the sentence\n",
    "doc = nlp(\"Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\")\n",
    "# Convert doc to a pandas object\n",
    "df = doc._.pandas\n",
    "\n",
    "# Select the columns accoroding to Malt-Tab format\n",
    "tmp = df[[\"form\", 'xpostag', 'head', 'deprel']].to_string(header=False, index=False)\n",
    "\n",
    "# See the outcome\n",
    "print(tmp)\n",
    "\n",
    "# Get finally our the DepencecyGraph\n",
    "dp = DependencyGraph(tmp)\n",
    "print('Tree:')\n",
    "dp.tree().pretty_print(unicodelines=True, nodedist=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
