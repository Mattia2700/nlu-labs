{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - different methods of representing words as vectors\n",
    "    - vectors and similarity between vectors\n",
    "    - evaluation of word embeddings\n",
    "    \n",
    "- Learning how to:\n",
    "    - train word embeddings with gensim\n",
    "    - use pre-trained word embeddings for similarity computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/)\n",
    "- tqdm\n",
    "- matplotlib\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "\n",
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/6.pdf) Chapter 6: Vector Semantics and Embeddings\n",
    "\n",
    "__Requirements__\n",
    "\n",
    "- spaCy\n",
    "- [gensim](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Words as Vectors (Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), [**word embedding**](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity / usage: similar words are close in \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Co-Occurence Matrices and Word as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs\n",
    "- $N$ is the total number of documents in the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- \"the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.1. Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.2. Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.3. PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Training Word Embeddings with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.1. Word2Vec\n",
    "- dense vectors\n",
    "- representation is created by training a classifier to distinguish nearby and far-away words\n",
    "- Variants\n",
    "    - SKIP-GRAM\n",
    "    - CBOW\n",
    "- Refer to [documentation](https://radimrehurek.com/gensim/models/word2vec.html) for details\n",
    "- [Tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: Levenshtein==0.21.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from python-Levenshtein) (0.21.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from Levenshtein==0.21.0->python-Levenshtein) (3.0.0)\n",
      "Requirement already satisfied: gensim in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from gensim.models import Word2Vec\n",
    "data = ['Iceland is faraway from Padova', 'Rome is the capital of Italy', 'Paris is a big city']\n",
    "model = Word2Vec(sentences=[d.split() for d in data], vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=14, vector_size=10, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01631476  0.00189917  0.03473637  0.00217777  0.09618826  0.05060603\n",
      " -0.0891739  -0.0704156   0.00901456  0.06392534]\n",
      "[('faraway', 0.5111488699913025), ('Italy', 0.2914133667945862), ('Iceland', 0.07346682250499725)]\n"
     ]
    }
   ],
   "source": [
    "# getting word vectors\n",
    "print(model.wv['Rome'])\n",
    "# getting most similar\n",
    "print(model.wv.most_similar('Rome', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Vector Similarity\n",
    "- two words are similar in meaning if their context __vectors__ are similar\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "- Implement a function to compute __cosine similarity__ using `numpy` methods\n",
    "    - `np.dot`\n",
    "    - `norm`\n",
    "- Using the defined functions\n",
    "    - compute cosine similarity between two word embeddings for instance `Rome` and `city` or `Paris` and `Rome`\n",
    "    - compare similarity values to the cosine similarity using the ouput of (`scipy.spatial.distance.cosine`)\n",
    "        - i.e. use *distance* to compute *similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04265024\n",
      "0.0426502488553524\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v,w)/(norm(v)*norm(w))\n",
    "\n",
    "rome = model.wv['Rome']\n",
    "paris = model.wv['Paris']\n",
    "print(cosine_similarity(rome, paris))\n",
    "print(1 - cosine(rome, paris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Pre-Trained Embeddings\n",
    "- Training embeddings is computationally expensive\n",
    "- Many pre-trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "# w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Word Embeddings in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> To make them compact and fast, spaCy's small pipeline packages (all packages that end in `sm`) don't ship with word vectors, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens -- but the result won't be as good, and individual tokens won't have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n",
    "> Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an __average of their token vectors__. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "> Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.7/587.7 MB 1.4 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from en-core-web-lg==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/disi/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spacy\u001b[39m.\u001b[39;49mcli\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_lg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages/spacy/cli/download.py:75\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(model, direct, sdist, *pip_args)\u001b[0m\n\u001b[1;32m     71\u001b[0m     version \u001b[39m=\u001b[39m get_version(model_name, compatibility)\n\u001b[1;32m     73\u001b[0m filename \u001b[39m=\u001b[39m get_model_filename(model_name, version, sdist)\n\u001b[0;32m---> 75\u001b[0m download_model(filename, pip_args)\n\u001b[1;32m     76\u001b[0m msg\u001b[39m.\u001b[39mgood(\n\u001b[1;32m     77\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDownload and installation successful\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can now load the package via spacy.load(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages/spacy/cli/download.py:131\u001b[0m, in \u001b[0;36mdownload_model\u001b[0;34m(filename, user_pip_args)\u001b[0m\n\u001b[1;32m    129\u001b[0m pip_args \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(user_pip_args) \u001b[39mif\u001b[39;00m user_pip_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    130\u001b[0m cmd \u001b[39m=\u001b[39m [sys\u001b[39m.\u001b[39mexecutable, \u001b[39m\"\u001b[39m\u001b[39m-m\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minstall\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m pip_args \u001b[39m+\u001b[39m [download_url]\n\u001b[0;32m--> 131\u001b[0m run_command(cmd)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages/spacy/util.py:997\u001b[0m, in \u001b[0;36mrun_command\u001b[0;34m(command, stdin, capture)\u001b[0m\n\u001b[1;32m    995\u001b[0m     cmd_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(command)\n\u001b[1;32m    996\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m     ret \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    998\u001b[0m         cmd_list,\n\u001b[1;32m    999\u001b[0m         env\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m   1000\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mstdin,\n\u001b[1;32m   1001\u001b[0m         encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1002\u001b[0m         check\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1003\u001b[0m         stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE \u001b[39mif\u001b[39;49;00m capture \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1004\u001b[0m         stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT \u001b[39mif\u001b[39;49;00m capture \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m     \u001b[39m# Indicates the *command* wasn't found, it's an error before the command\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# is run.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         Errors\u001b[39m.\u001b[39mE970\u001b[39m.\u001b[39mformat(str_command\u001b[39m=\u001b[39mcmd_str, tool\u001b[39m=\u001b[39mcmd_list[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1011\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:495\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    494\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    496\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    497\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1020\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1020\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   1021\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1022\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1083\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     endtime \u001b[39m=\u001b[39m _time() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m   1082\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1084\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m     \u001b[39m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1806\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1805\u001b[0m     \u001b[39mbreak\u001b[39;00m  \u001b[39m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1806\u001b[0m (pid, sts) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_wait(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1807\u001b[0m \u001b[39m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m \u001b[39m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m \u001b[39m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m \u001b[39mif\u001b[39;00m pid \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1764\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1764\u001b[0m     (pid, sts) \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, wait_flags)\n\u001b[1;32m   1765\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1766\u001b[0m     \u001b[39m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     \u001b[39m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     \u001b[39m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1769\u001b[0m     pid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.1. Accessing Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Rome\n",
      "vector dimension: 300\n",
      "spacy vector norm: 54.82853\n",
      "numpy vector norm: 54.82853\n",
      "numpy linalg norm: 54.82853\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "txt = 'Rome is the capital of Italy'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's get Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "# print(\"vector:\", tok.vector)\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)\n",
    "print(\"numpy vector norm:\", np.sqrt(np.dot(tok.vector, tok.vector)))\n",
    "print(\"numpy linalg norm:\", np.linalg.norm(tok.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "spacy CosSim(Rome, Paris): 0.6117808222770691\n",
      "scipy CosSim(Rome, Paris): 0.6117808222770691\n",
      "_our_ CosSim(Rome, Paris): 0.6117808\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('Paris')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))\n",
    "print(\"_our_ CosSim({}, {}):\".format(tok.text, paris.text), cosine_similarity(tok.vector, paris.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train your own Word Embeddings\n",
    "One way to train word embeddings is to use a language model. We have already seen language models in Lab 3, but now we are going to develop a language model using a neural architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Task definition\n",
    "To model the probaiblity distribution over a sequence, we are going to use the Chain Rule as we have seen in LAB 3:\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "However, at that time we have used ngram to trucate the previous context ($N-1$), in order to compute meaningfull probabilities. While using neural models, we will let the model to decide by itself how to manage the previous contex and thus which are the tokens relevant for the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 RNNs are the most suitable architacture\n",
    "One of most suitable neural architecture for the Language Model task is the Recurrent Neural Network. The architecture is composed of a RNN layer (vanilla, LSTM, GRU) and a softmax that outputs the probability over the dictionary. Indeed the size of the output vector is equal to the size of the dictionary, i.e. the model cannot predict tokens that are not present in vocabularly. <br>\n",
    "> LM task in RNN can be tackled as a sequence labelling task (i.e. len of input and output sequence are always the same) in which the input sequence is $ input = \\{w_1, w_2, w_{n-1}\\}$ and the output is $ output = \\{w_2, w_2, w_{n}\\}$\n",
    ">\n",
    "> **Example** our sentence is ***\"I go to Miami\"*** the input sequence would be ***\"I go to\"*** and the output is ***\"go to Miami\"***. \n",
    ">\n",
    "> Notice: \n",
    "> - To proper model the sequence probabilities we need add boundary markers \\<s\\> and \\</s\\>.\n",
    "> - However in LM RNN only the end of sentence token \\</s\\>  is usually used unless we need for some reason (e.g. in ASR) to compute the probability distribution of the first token of a sentence. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/zGH99MFY/rnn-lm.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "In the image below you can see a working example of a language model with RNN. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/fydQNrYP/LM-RNN.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nlu-labs-SJy_Cgav-py3.8/lib/python3.8/site-packages/torch/__init__.py:457\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(textwrap\u001b[39m.\u001b[39mdedent(\u001b[39m'''\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[39m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[39m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[39m            \u001b[39m\u001b[39m'''\u001b[39m)\u001b[39m.\u001b[39mstrip()) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m  \u001b[39m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(_C):\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m name[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mBase\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    459\u001b[0m         __all__\u001b[39m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficienty purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(x + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Data loading \n",
    "For sake of time we are going to see this part in detail in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# However you can compute it for dev and test just for statistics about OOV \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Train and validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 243.882809: 100%|██████████████████████████| 29/29 [02:55<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  228.9181227468783\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print('Test ppl: ', final_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7 Evaluation: Analogy Task\n",
    "In the word analogy task, we complete the sentence of the form\n",
    "\n",
    "\"$w_1$ is to $w_2$ as $w_3$ is to $w4$\", where $w_4$ is a blank. \n",
    "\n",
    "For instance:\n",
    "\n",
    "\"*man* is to *woman* as *king* is to **__**\", and our goal is to guess the missing word (*queen*)\n",
    "\n",
    "The task is approached using cosine similarity between vector differences: \n",
    "\n",
    "$$\\vec{w_2} - \\vec{w_1} \\approx \\vec{w_4} - \\vec{w_3}$$\n",
    "\n",
    "$$\\vec{w_4} \\approx = \\vec{w_3} + \\vec{w_2} - \\vec{w_1}$$\n",
    "\n",
    "$$w = \\arg\\max_{w \\in V}(\\vec{w} \\cdot (\\vec{w_3} + \\vec{w_2} - \\vec{w_1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w = \\arg\\max_{w \\in V}\\text{CosSim}(\\vec{w_2} - \\vec{w_1}, \\vec{w} - \\vec{w_3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Analogy using Most Similar\n",
    "> For each of the given vectors, find the `n` most similar entries to it by cosine. \n",
    "Queries are by vector. Results are returned as a (`keys`, `best_rows`, `scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_spacy(w1, w2, w3):\n",
    "    v1 = nlp.vocab[w1].vector\n",
    "    v2 = nlp.vocab[w2].vector\n",
    "    v3 = nlp.vocab[w3].vector\n",
    "    \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "   \n",
    "    # n=1 & sorted by default\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([rv]), n=10)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0][0]):\n",
    "        print(nlp.vocab.strings[key], ms[2][0][i])\n",
    "        \n",
    "def analogy_our_model(w1, w2, w3, model, lang):\n",
    "    model.eval().to('cpu')\n",
    "    tmp_w1 = torch.LongTensor([lang.word2id[w1] if w1 in lang.word2id else lang.word2id['<unk>']]) \n",
    "    tmp_w2 = torch.LongTensor([lang.word2id[w2] if w2 in lang.word2id else lang.word2id['<unk>']])\n",
    "    tmp_w3 = torch.LongTensor([lang.word2id[w3] if w3 in lang.word2id else lang.word2id['<unk>']])\n",
    "    \n",
    "    v1 = model.get_word_embedding(tmp_w1)\n",
    "    v2 = model.get_word_embedding(tmp_w2)\n",
    "    v3 = model.get_word_embedding(tmp_w3)\n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "\n",
    "    # n=1 & sorted by default\n",
    "    ms = model.get_most_similar(rv)\n",
    "\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0]):\n",
    "        print(lang.id2word[key], ms[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stakes 0.562926\n",
      "argue 0.5512035\n",
      "pepsi 0.2334832\n",
      "gerard 0.22715671\n",
      "beating 0.22661649\n",
      "bourbon 0.21993926\n",
      "produces 0.21790373\n",
      "prepare 0.18623923\n",
      "write-offs 0.18256277\n",
      "deployed 0.18126944\n"
     ]
    }
   ],
   "source": [
    "analogy_our_model('man', 'woman', 'u.s.', model, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[13176088972490086564, 14826469074451677028,  7464393751932445219,\n",
      "         7102492827649024548, 10168488388102651113,  4176741725343376093,\n",
      "         5247273317732208552, 11742085837932180620, 12278543830867659210,\n",
      "         4527521648030784477]], dtype=uint64), array([[391587,   2182,   3149,  27269,   5309,   6025,  59855,  94888,\n",
      "         11899,   7473]], dtype=int32), array([[0.8024, 0.8024, 0.8024, 0.8024, 0.7881, 0.7881, 0.7881, 0.6401,\n",
      "        0.6401, 0.6401]], dtype=float32))\n",
      "KIng 0.8024\n",
      "King 0.8024\n",
      "king 0.8024\n",
      "KING 0.8024\n",
      "Queen 0.7881\n",
      "queen 0.7881\n",
      "QUEEN 0.7881\n",
      "PRINCE 0.6401\n",
      "prince 0.6401\n",
      "Prince 0.6401\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy_spacy('man', 'woman', 'king'))\n",
    "# expected output ('Queen', 0.7881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(analogy_spacy('Rome', 'Italy', 'Paris'))\n",
    "# ('france', 0.7606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1 (2 points)\n",
    "Modify the baseline LM_RNn (the idea is to add a set of improvements and see how these affect the performance):\n",
    "- Replace RNN with LSTM (output the PPL)\n",
    "- Add two dropout layers: (output the PPL)\n",
    "    - one on embeddings, \n",
    "    - one on the output\n",
    "- Replace SGD with AdamW (output the PPL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output\n",
    "\n",
    "train_raw = read_file(\"ptb.train.txt\")\n",
    "dev_raw = read_file(\"ptb.valid.txt\")\n",
    "test_raw = read_file(\"ptb.test.txt\")\n",
    "\n",
    "# Vocab is computed only on training set \n",
    "# However you can compute it for dev and test just for statistics about OOV \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "\n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "\n",
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "print(vocab_len)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        self.dropout = dropout\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout)\n",
    "        mask = Variable(m, requires_grad=False) / (1 - self.dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.3,\n",
    "                 emb_dropout=0.3, n_layers=1, lstm=False, dropout=False, tie_weights=False, variational=False):\n",
    "        self.dropout = dropout\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        if tie_weights:\n",
    "            if emb_size != hidden_size:\n",
    "                print(\"When tying weights, emb_size must be equal to hidden_size. Setting hidden_size to emb_size\")\n",
    "                hidden_size = emb_size\n",
    "\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        if self.dropout:\n",
    "            if variational:\n",
    "                self.embedding_dropout = VariationalDropout(emb_dropout)\n",
    "            else:\n",
    "                self.embedding_dropout = nn.Dropout(emb_dropout)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        if lstm:\n",
    "            self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)   \n",
    "        self.pad_token = pad_index\n",
    "        if self.dropout:\n",
    "            if variational:\n",
    "                self.rnn_dropout = VariationalDropout(out_dropout)\n",
    "            else:\n",
    "                self.rnn_dropout = nn.Dropout(out_dropout)\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            self.output.weight = self.embedding.weight\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        if self.dropout:\n",
    "            emb = self.embedding_dropout(emb)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        if self.dropout:\n",
    "            rnn_out = self.rnn_dropout(rnn_out)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def train(lstm=False, droput=False, adamW=False, tie_weights=False, variational=False):\n",
    "\n",
    "    model = LM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], lstm=lstm, dropout=droput, tie_weights=tie_weights, variational=variational).to(device)\n",
    "    model.apply(init_weights)\n",
    "    if adamW:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    n_epochs = 100\n",
    "    patience = 5\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_ppl = math.inf\n",
    "    best_model = None\n",
    "    pbar = tqdm(range(1,n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sampled_epochs.append(epoch)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "            if  ppl_dev < best_ppl: # the lower, the better\n",
    "                best_ppl = ppl_dev\n",
    "                best_model = copy.deepcopy(model).to('cpu')\n",
    "                patience = 5\n",
    "            else:\n",
    "                patience -= 1\n",
    "                \n",
    "            if patience <= 0: # Early stopping with patience\n",
    "                break # Not nice but it keeps the code clean\n",
    "                            \n",
    "    best_model.to(device)\n",
    "    final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "    print('Test ppl: ', final_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 464.051675: 100%|██████████| 99/99 [19:18<00:00, 11.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  449.81199071603294\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 343.966438:  61%|██████    | 60/99 [13:21<08:40, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  327.2894307904623\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True) # lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 548.239366: 100%|██████████| 99/99 [22:41<00:00, 13.75s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  530.1939353391774\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True) # lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 385.028345:  56%|█████▌    | 55/99 [12:15<09:48, 13.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  364.14453891687936\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True) # lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 583.456062: 100%|██████████| 99/99 [22:37<00:00, 13.72s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  565.04140115898\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True) # lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 9736.689567: 100%|██████████| 99/99 [22:56<00:00, 13.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  9731.80877605446\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 308.994419:   8%|▊         | 8/99 [01:57<22:22, 14.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  279.27786405396733\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True, adamW=True) # lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 201.957143:  11%|█         | 11/99 [02:45<22:02, 15.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  178.42669291455206\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True, adamW=True) # lr=0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original PPL: 9137.844126625321\n",
    "- LSTM PPL: 530.1939353391774\n",
    "- LSTM + Dropout PPL: 565.04140115898\n",
    "- LSTM + Dropout + AdamW PPL: 182.56665051308198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (4 points)\n",
    "Add the following regularizations described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ):\n",
    "- Weight Tying\n",
    "- Variational Dropout\n",
    "- Non-monotonically Triggered AvSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When tying weights, emb_size must be equal to hidden_size. Setting hidden_size to emb_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 181.934409:  12%|█▏        | 12/99 [03:11<23:06, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  158.99414263681433\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True, adamW=True, tie_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When tying weights, emb_size must be equal to hidden_size. Setting hidden_size to emb_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 267.964474:  11%|█         | 11/99 [02:58<23:48, 16.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  238.88784105216754\n"
     ]
    }
   ],
   "source": [
    "train(lstm=True, droput=True, adamW=True, tie_weights=True, variational=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NT_AvSGD(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, w0, L, n, model, lr=0.1):\n",
    "        super(NT_AvSGD, self).__init__(params, dict(lr=lr))\n",
    "        self.params = params\n",
    "        self.w0 = w0\n",
    "        self.lr = lr\n",
    "        self.L = L\n",
    "        self.n = n\n",
    "        self.model = model\n",
    "        self.sgd = optim.SGD(self.params, lr=self.lr)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        k = 0\n",
    "        t = 0\n",
    "        T = 0\n",
    "        logs = []\n",
    "        while True:\n",
    "            self.sgd.step(closure)\n",
    "            if k % self.logging_interval == 0 and T == 0:\n",
    "                ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, self.model)\n",
    "                \n",
    "                if t > self.n and ppl_dev > min(logs[:t-self.n-1]):\n",
    "                    T = k\n",
    "\n",
    "                logs.append(ppl_dev)\n",
    "            \n",
    "            k += 1\n",
    "\n",
    "    return sum()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM + Dropout + AdamW + Weight Tying PPL: 158.99414263681433 (0.01)\n",
    "- LSTM + Dropout + AdamW + Weight Tying + Variational Dropout PPL: 159.6664387721116 (0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
