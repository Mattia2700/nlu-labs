{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mattia2700/nlu-labs/blob/main/01_corpus_and_lexicon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfIP1dFYsLl_"
   },
   "source": [
    "# Corpus and Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D129BhxssLmB"
   },
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - relation between corpus and lexicon\n",
    "    - effects of pre-processing (tokenization) on lexicon\n",
    "    \n",
    "- Learning how to:\n",
    "    - load basic corpora for processing\n",
    "    - compute basic descriptive statistic of a corpus\n",
    "    - building lexicon and frequency lists from a corpus\n",
    "    - perform basic lexicon operations\n",
    "    - perform basic text pre-processing (tokenization and sentence segmentation) using python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgYiMy9ksLmD"
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2njKSn8sLmD"
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 2: Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "    - [Chapter 3: Processing Raw Text](https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mZcXW4xsLmE"
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "    \n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English language model (`spacy>=3.0`)\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WJlNXGSsLmE"
   },
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxIOLUwvsLmF"
   },
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f77TDVf6sLmF"
   },
   "source": [
    "#### 1.1.1. Text Corpora in NLTK\n",
    "NLTK provides several corpora with loading functions. Plain text corpora come from a _Project Gutenberg_.\n",
    "\n",
    "`nltk.corpus.gutenberg.fileids()` lists available books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5y1Dk_q-sLmF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPfcoRuIsLmH"
   },
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJNd2UqSsLmH"
   },
   "source": [
    "#### 1.1.2. Units of Text Corpus\n",
    "Depending on a goal, corpus can be seen as a sequence of:\n",
    "- characters\n",
    "- words (tokens)\n",
    "- sentences\n",
    "- paragraphs\n",
    "- document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVRTI6nCsLmH"
   },
   "source": [
    "Each level, in turn, can be seen as a sequence of elements of the previous level.\n",
    "\n",
    "- word -- a sequence of characters\n",
    "- sentence -- a sequence of words\n",
    "- paragraph -- a sequence of sentences\n",
    "- document -- a sequence of paragraphs (or sentences, depending on our purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQP8vpQLsLmI"
   },
   "source": [
    "#### 1.1.3. Loading NLTK Corpora\n",
    "\n",
    "NLTK provides functions to load a corpus using these different levels, as `raw` (characters), `words`, and `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7rOmwFSsLmI"
   },
   "outputs": [],
   "source": [
    "alice_chars = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "print('chars:', alice_chars[0])\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "print('words:', alice_words[0])\n",
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "print('sents:', alice_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqYrY0CysLmI"
   },
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* can be described in terms of:\n",
    "\n",
    "- total number of characters\n",
    "- total number of words (_tokens_: includes punctuation, etc.)\n",
    "- total number of sentences\n",
    "\n",
    "- minimum/maximum/average number of character per token\n",
    "- minimum/maximum/average number of words per sentence\n",
    "- minimum/maximum/average number of sentences per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Example__\n",
    "\n",
    "$$\\text{Av. Token Count} = \\frac{\\text{count}(tokens)}{\\text{count}(sentences)}$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUyI07SDsLmJ"
   },
   "outputs": [],
   "source": [
    "# let's compute average sentence length & round to closes integer\n",
    "round(len(alice_words)/len(alice_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiXO1AFAsLmK"
   },
   "outputs": [],
   "source": [
    "# let's compute length of each sentence\n",
    "sent_lens = [len(sent) for sent in alice_sents]\n",
    "# let's compute length of each word\n",
    "word_lens = [len(word) for word in alice_words]\n",
    "# let's compute length the number of characters in each sentence\n",
    "chars_lens = [len(''.join(sent)) for sent in alice_sents]\n",
    "\n",
    "avg_sent_len = round(sum(sent_lens)/len(sent_lens))\n",
    "min_sent_len = min(sent_lens)\n",
    "max_sent_len = max(sent_lens)\n",
    "print(\"AVG sent len\", avg_sent_len)\n",
    "print(\"MIN sent len\", min_sent_len)\n",
    "print(\"MAX sent len\", max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUDRtqDcsLmK"
   },
   "outputs": [],
   "source": [
    "# JOIN built-in function example\n",
    "tmp = ['H', 'e', 'l', 'l', 'o']\n",
    "print(''.join(tmp))\n",
    "print('‚≠ê'.join(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuUizaxdsLmK"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Define a function to compute corpus descriptive statistics\n",
    "\n",
    "    - input:\n",
    "        - raw text (Chars)\n",
    "        - words\n",
    "        - sentences\n",
    "    - output (print): \n",
    "        - average number of:\n",
    "            - chars per word\n",
    "            - words per sentence\n",
    "            - chars per sentence\n",
    "        - Size of the longest word and sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TazZzM2bsLmK"
   },
   "outputs": [],
   "source": [
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(word) for word in words]\n",
    "    sent_lens = [len(sent) for sent in sents]\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in sents]\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sents)\n",
    "    longest_word = max(words)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longeset_word = statistics(alice_chars, alice_words, alice_sents)\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sent)\n",
    "print('Longest word', longeset_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDzJRSNgsLmL"
   },
   "source": [
    "## 2. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUchXRtPsLmL"
   },
   "source": [
    "### 2.1. Lexicon and Its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuhP1BpvsLmL"
   },
   "source": [
    "#### 2.1.1. Constructing Lexicon and Computing its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_iHWZrosLmL"
   },
   "source": [
    "Since lexicon is a list of unique elemets, it is a `set` of corpus words (i.e. tokens).\n",
    "Consequently, its size is the size of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_wJ6JvgsLmM"
   },
   "outputs": [],
   "source": [
    "alice_lexicon = set(alice_words)\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZavjgT5UsLmM"
   },
   "source": [
    "__NOTE__:\n",
    "We did not process our corpus in any way. Consequently, words with case variations are different entries in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zbCyDPssLmM"
   },
   "outputs": [],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djDY1UTzsLmM"
   },
   "source": [
    "#### 2.1.2. Lowercased Lexicon\n",
    "Let's lowercase our corpus and re-compute the lexicon size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R_Zbs4jsLmM"
   },
   "outputs": [],
   "source": [
    "alice_lexicon = set([w.lower() for w in alice_words])\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VuUmNgCsLmM"
   },
   "outputs": [],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcdSoqMNsLmN"
   },
   "source": [
    "### 2.2. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXPGNPBPsLmN"
   },
   "source": [
    "#### 2.2.1. Computing Frequency List with python\n",
    "\n",
    "In python, frequency list can be constructed in several ways. The most convenient is the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P32f-T-OsLmN"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "alice_freq_list = Counter(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KViqpJGKsLmN"
   },
   "outputs": [],
   "source": [
    "print(alice_freq_list.get('ALL', 0))\n",
    "print(alice_freq_list.get('All', 0))\n",
    "print(alice_freq_list.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMNCiAO3sLmO"
   },
   "source": [
    "#### 2.2.2. Computing Frequency List with NLTK\n",
    "NLTK provides `FreqDist` class to construct a Frequency List (`FreqDist` == _Frequency Distribution_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSBS8RrLsLmO"
   },
   "outputs": [],
   "source": [
    "alice_freq_dist = nltk.FreqDist(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSz7n1JcsLmO"
   },
   "outputs": [],
   "source": [
    "print(alice_freq_dist.get('ALL', 0))\n",
    "print(alice_freq_dist.get('All', 0))\n",
    "print(alice_freq_dist.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUIxiT-7sLmO"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- compute frequency list of __lowercased__ \"alice\" corpus (you can use either method)\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| ,      |     1,993 |\n",
    "| '      |     1,731 |\n",
    "| the    |     1,642 |\n",
    "| and    |       872 |\n",
    "| .      |       764 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_GTLXolsLmO"
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)# Counter(X) # Replace X with the word list of the corpus in lower case (see above))\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yAn6KNOsLmP"
   },
   "outputs": [],
   "source": [
    "alice_lowercase_freq_list = Counter([w.lower() for w in alice_words]) # Counter(X) # Replace X with the word list of the corpus in lower case (see above))\n",
    "nbest(alice_lowercase_freq_list, n=5) # Change N form 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSzb0eJgsLmP"
   },
   "source": [
    "### 2.3. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QPx8qdLsLmP"
   },
   "source": [
    "#### 2.3.1. Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF6cWMVQsLmP"
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "<!-- - define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "    \n",
    "    - input: frequence list (dict)\n",
    "    - output: list\n",
    "    - use default values for min and max\n",
    "     -->\n",
    "- Using the function cut_off\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table (on the lowercased lexicon)\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 2636 |\n",
    "| cut-off    |   2 | N/A | 1503 |\n",
    "| cut-off    | N/A | 100 | 2586 |\n",
    "| cut-off    |   2 | 100 | 1453 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fquNDc6sLmP"
   },
   "outputs": [],
   "source": [
    "def cut_off(vocab, n_min=100, n_max=100):\n",
    "    new_vocab = []\n",
    "    for word, count in vocab.items():\n",
    "        if count >= n_min and count <= n_max:\n",
    "            new_vocab.append(word)\n",
    "    return new_vocab\n",
    "\n",
    "lower_bound = float(\"2\") # Change these two number to compute the required cut offs\n",
    "upper_bound = float(\"100\")\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iyoc28DsLmP"
   },
   "source": [
    "#### 2.3.2. StopWord Removal\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG0z4ULSsLmP"
   },
   "source": [
    "Let's check the stop word lists from the popular python libraries.\n",
    "\n",
    "- spaCy\n",
    "- NLTK\n",
    "- scikit-learn\n",
    "\n",
    "    \n",
    "For NLTK we need to download them first\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec4_NzsysLmQ"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkxulsjqsLmQ"
   },
   "source": [
    "##### Exercise\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    - compute the intersection between the 100 most frequent words in frequency list of the alice corpus and the list of stopwords (report count)\n",
    "    - remove stopwords from the lexicon\n",
    "    - print the size of:\n",
    "            - original lexicon\n",
    "            - lexicon without stopwords\n",
    "            - overlap between 100 most freq. words and stopwords\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 2636 |\n",
    "| no stop words   | 2490 |\n",
    "| top 100 overlap |   65 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbof4UdfsLmQ"
   },
   "outputs": [],
   "source": [
    "# Set built-in Function\n",
    "set_a = set(['a', 'b', 'c', 'd', 'e'])\n",
    "set_b = set(['a', 'b', 'f'])\n",
    "\n",
    "print(set_a.intersection(set_b)) # Compute overlap\n",
    "print(set_a.difference(set_b)) # Remove Elements by computing the set diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMGACtqIsLmQ"
   },
   "outputs": [],
   "source": [
    "alice_vocab = set([w.lower() for w in alice_words])\n",
    "top100 = list(nbest(alice_lowercase_freq_list,n=100).keys())\n",
    "stop_words = NLTK_STOP_WORDS\n",
    "overlap = set(top100).intersection(stop_words)\n",
    "alice_vocab_no_stopwords = alice_vocab.difference(stop_words)\n",
    "print('Original', len(alice_vocab))\n",
    "print('No stopwords', len(alice_vocab_no_stopwords))\n",
    "print('To100 overlap', len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjApkVzEsLmR"
   },
   "source": [
    "## 3. Basic Text Pre-processing\n",
    "\n",
    "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for tranforming text for Machine Learning tasks.\n",
    "\n",
    "__Text Normalization Steps__\n",
    "\n",
    "- removing extra white spaces\n",
    "\n",
    "- tokenization\n",
    "    - documents to sentences (sentence segmentation/tokenization)\n",
    "    - sentences to tokens\n",
    "\n",
    "- lowercasing/uppercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONE65bJnsLmR"
   },
   "source": [
    "- removing punctuation\n",
    "\n",
    "- removing accent marks and other diacritics \n",
    "\n",
    "- removing stop words (see above)\n",
    "\n",
    "- removing sparse terms (frequency cut-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOo3kNawsLmR"
   },
   "source": [
    "- number normalization\n",
    "    - numbers to words (i.e. `10` to `ten`)\n",
    "    - number words to numbers (i.e. `ten` to `10`)\n",
    "    - removing numbers\n",
    "\n",
    "- verbalization (specifically for speech applications)\n",
    "\n",
    "    - numbers to words\n",
    "    - expanding abbreviations (or spelling out)\n",
    "    - reading out dates, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLfpRgBAsLmR"
   },
   "source": [
    "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
    "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form‚Äîgenerally a written word form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gir7bxtYsLmR"
   },
   "source": [
    "### 3.1. Tokenization and Sentence Segmentation\n",
    "\n",
    "Given a \"clean\" text, in order to allow any analysis, we need to identify its units.\n",
    "In other words, we need to _segment_ the text into sentences and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWLwFvrRsLmR"
   },
   "source": [
    "__NOTE__:\n",
    "Since both _tokenization_ and _sentence segmentation_ are automatic, different tools yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-vhWNE4sLmR"
   },
   "source": [
    "#### 3.1.1. Tokenization and Sentence Segmentation with spaCy\n",
    "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, lemmatization, *dependency parsing* and *Named Entity Recognition* (ignore the ones in *italics* for today). \n",
    "\n",
    "\n",
    "SpaCy produces a `Doc` object that contains `Span`s (sentences) and `Token`s (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAdsmi3vsLmR"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "# un-comment the lines above, if you get 'ModuleNotFoundError'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txt = alice_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laYJOvSWsLmR"
   },
   "outputs": [],
   "source": [
    "# process the document\n",
    "doc = nlp(txt, disable=[\"tagger\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PScQhVdsLmS"
   },
   "outputs": [],
   "source": [
    "print(\"first token: '{}'\".format(doc[0]))\n",
    "print(\"first sentence: '{}'\".format(list(doc.sents)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvXVL5XLsLmS",
    "outputId": "7d6f5af8-07d9-427c-8abf-147d55c48c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37033\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# access list of tokens (Token objects)\n",
    "print(len(doc))\n",
    "# access list of sentences (Span objects)\n",
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSIqBvE6sLmS"
   },
   "source": [
    "#### 3.1.2. Tokenization and Sentence Segmentation with NLTK\n",
    "NLTK's [tokenize](https://www.nltk.org/api/nltk.tokenize.html) package provides similar functionality using the methods below.\n",
    "\n",
    "- `word_tokenize` \n",
    "- `sent_tokenize`\n",
    "\n",
    "There are several tokenizer available (read documentation for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HVKUHhtisLmS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39m# download NLTK tokenizer\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m nltk\u001B[39m.\u001B[39mdownload(\u001B[39m'\u001B[39m\u001B[39mpunkt\u001B[39m\u001B[39m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pd60bhHsLmT"
   },
   "outputs": [],
   "source": [
    "alice_words_nltk = nltk.word_tokenize(alice_chars)\n",
    "alice_sents_nltk = nltk.sent_tokenize(alice_chars)\n",
    "print(len(alice_words_nltk))\n",
    "print(len(alice_sents_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmpQH3pdsLmT"
   },
   "outputs": [],
   "source": [
    "print(\"first token: '{}'\".format(alice_words_nltk[0]))\n",
    "print(\"first sentence: '{}'\".format(alice_sents_nltk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnK-8bwGsLmT"
   },
   "source": [
    "## Lab Exercise\n",
    "- Load another corpus from Gutenberg (e.g. `milton-paradise.txt`)\n",
    "- Compute descriptive statistics on the __reference__ sentences and tokens.\n",
    "- Compute descriptive statistics in the __automatically__ processed corpus\n",
    "    - both with `spacy` and `nltk`\n",
    "- Compute lowercased lexicons for all 3 versions of the corpus\n",
    "    - compare lexicon sizes\n",
    "- Compute frequency distribution for all 3 versions of the corpus\n",
    "    - compare top N frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrGLogVOsZFK"
   },
   "source": [
    "### Install and import libraries, download models and define custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPtyDjcMs2cQ"
   },
   "outputs": [],
   "source": [
    "!pip install nltk spacy scikit-learn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo0fU2vmshl-"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoJopD5SsLmT"
   },
   "outputs": [],
   "source": [
    "# model = 'bible-kjv.txt'\n",
    "model = 'milton-paradise.txt'\n",
    "\n",
    "nlp.max_length = 4332554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qZQ7SdDvGNs"
   },
   "outputs": [],
   "source": [
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(word) for word in words]\n",
    "    sent_lens = [len(sent) for sent in sents]\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in sents]\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sents)\n",
    "    longest_word = max(words)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL5tobmfuDwj"
   },
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtrmb7Jwm_W"
   },
   "source": [
    "#### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLf9Tv2OsLmT",
    "outputId": "e739fdbd-73f2-4fcb-e48e-795874c34605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 468220\n",
      "Number of words: 96825\n",
      "Number of sentences: 1851\n",
      "Minimum - Maximum - Average number of characters per token: 1 - 16 - 4\n",
      "Minimum - Maximum - Average number of words per sentence: 2 - 533 - 52\n",
      "Minimum - Maximum - Average number of sentences per document: 2 - 533 - 52\n"
     ]
    }
   ],
   "source": [
    "chars = nltk.corpus.gutenberg.raw(model)\n",
    "words = nltk.corpus.gutenberg.words(model)\n",
    "sents = nltk.corpus.gutenberg.sents(model)\n",
    "\n",
    "# minimum/maximum/average number of character per token\n",
    "min_char_per_token = min([len(w) for w in words])\n",
    "max_char_per_token = max([len(w) for w in words])\n",
    "avg_char_per_token = round(sum([len(w) for w in words]) / len(words))\n",
    "# minimum/maximum/average number of words per sentence\n",
    "min_word_per_sent = min([len(s) for s in sents])\n",
    "max_word_per_sent = max([len(s) for s in sents])\n",
    "avg_word_per_sent = round(sum([len(s) for s in sents]) / len(sents))\n",
    "# minimum/maximum/average number of sentences per document\n",
    "min_sent_per_doc = min([len(s) for s in sents])\n",
    "max_sent_per_doc = max([len(s) for s in sents])\n",
    "avg_sent_per_doc = round(sum([len(s) for s in sents]) / len(sents))\n",
    "\n",
    "print('Number of characters: {}'.format(len(chars)))\n",
    "print('Number of words: {}'.format(len(words)))\n",
    "print('Number of sentences: {}'.format(len(sents)))\n",
    "print('Minimum - Maximum - Average number of characters per token: {} - {} - {}'.format(min_char_per_token, max_char_per_token, avg_char_per_token))\n",
    "print('Minimum - Maximum - Average number of words per sentence: {} - {} - {}'.format(min_word_per_sent, max_word_per_sent, avg_word_per_sent))\n",
    "print('Minimum - Maximum - Average number of sentences per document: {} - {} - {}'.format(min_sent_per_doc, max_sent_per_doc, avg_sent_per_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ-0S_yRyHvl"
   },
   "source": [
    "#### Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-vnhKm0yQtq"
   },
   "source": [
    "- SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njkJmMLNsLmU",
    "outputId": "ed2a356e-d278-4e72-c4c3-065454a98f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 468220\n",
      "Number of words: 107373\n",
      "Number of sentences: 2092\n",
      "Minimum - Maximum - Average number of characters per token: 1 - 63 - 4\n",
      "Minimum - Maximum - Average number of words per sentence: 1 - 409 - 51\n",
      "Minimum - Maximum - Average number of sentences per document: 1 - 409 - 51\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(chars, disable=[\"tagger\", \"ner\", \"lemmatizer\"])\n",
    "words = [w for w in doc]\n",
    "sents = [s for s in doc.sents]\n",
    "\n",
    "# minimum/maximum/average number of character per token\n",
    "min_char_per_token = min([len(w) for w in words])\n",
    "max_char_per_token = max([len(w) for w in words])\n",
    "avg_char_per_token = round(sum([len(w) for w in words]) / len(words))\n",
    "# minimum/maximum/average number of words per sentence\n",
    "min_word_per_sents = min([len(s) for s in sents])\n",
    "max_word_per_sents = max([len(s) for s in sents])\n",
    "avg_word_per_sents = round(sum([len(s) for s in sents]) / len(sents))\n",
    "# minimum/maximum/average number of sentences per document\n",
    "min_sent_per_doc = min([len(s) for s in sents])\n",
    "max_sent_per_doc = max([len(s) for s in sents])\n",
    "avg_sent_per_doc = round(sum([len(s) for s in sents]) / len(sents))\n",
    "\n",
    "print('Number of characters: {}'.format(len(chars)))\n",
    "print('Number of words: {}'.format(len(words)))\n",
    "print('Number of sentences: {}'.format(len(sents)))\n",
    "print('Minimum - Maximum - Average number of characters per token: {} - {} - {}'.format(min_char_per_token, max_char_per_token, avg_char_per_token))\n",
    "print('Minimum - Maximum - Average number of words per sentence: {} - {} - {}'.format(min_word_per_sents, max_word_per_sents, avg_word_per_sents))\n",
    "print('Minimum - Maximum - Average number of sentences per document: {} - {} - {}'.format(min_sent_per_doc, max_sent_per_doc, avg_sent_per_doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqMSXMfG5s98"
   },
   "source": [
    "- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wToTFEaMsLmU",
    "outputId": "4f12505c-30a0-4590-8eb6-65aa15e78112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 468220\n",
      "Number of words: 95716\n",
      "Number of sentences: 1835\n",
      "Minimum - Maximum - Average number of characters per token: 1 - 18 - 4\n",
      "Minimum - Maximum - Average number of words per sentence: 2 - 2658 - 253\n",
      "Minimum - Maximum - Average number of sentences per document: 2 - 2658 - 253\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(chars)\n",
    "sents = nltk.sent_tokenize(chars)\n",
    "# minimum/maximum/average number of character per token\n",
    "min_char_per_token = min([len(w) for w in words])\n",
    "max_char_per_token = max([len(w) for w in words])\n",
    "avg_char_per_token = round(sum([len(w) for w in words]) / len(words))\n",
    "# minimum/maximum/average number of words per sentence\n",
    "min_word_per_sent = min([len(s) for s in sents])\n",
    "max_word_per_sent = max([len(s) for s in sents])\n",
    "avg_word_per_sent = round(sum([len(s) for s in sents]) / len(sents))\n",
    "# minimum/maximum/average number of sentences per document\n",
    "min_sent_per_doc = min([len(s) for s in sents])\n",
    "max_sent_per_doc = max([len(s) for s in sents])\n",
    "avg_sent_per_doc = round(sum([len(s) for s in sents]) / len(sents))\n",
    "\n",
    "print('Number of characters: {}'.format(len(chars)))\n",
    "print('Number of words: {}'.format(len(words)))\n",
    "print('Number of sentences: {}'.format(len(sents)))\n",
    "print('Minimum - Maximum - Average number of characters per token: {} - {} - {}'.format(min_char_per_token, max_char_per_token, avg_char_per_token))\n",
    "print('Minimum - Maximum - Average number of words per sentence: {} - {} - {}'.format(min_word_per_sent, max_word_per_sent, avg_word_per_sent))\n",
    "print('Minimum - Maximum - Average number of sentences per document: {} - {} - {}'.format(min_sent_per_doc, max_sent_per_doc, avg_sent_per_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krGRKD5k7RGE"
   },
   "source": [
    "### Lowercased lexicons"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0f6a27336aaee01e89dbcf0c09560b03a0a2e5cab33d45b7ca097caf17f318b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
