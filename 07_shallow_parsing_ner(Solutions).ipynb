{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling: Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding: \n",
    "    - relation between Sequence Labeling and Shallow Parsing\n",
    "    - IOB Notation\n",
    "    - Joint Segmentation and Classification\n",
    "    - Feature Engineering\n",
    "- Learning how to:\n",
    "    - use Named Entity Recognition in \n",
    "        - spacy\n",
    "        - NLTK\n",
    "    - train, test, and evaluate Conditional Random Fields models\n",
    "    - perform feature engineering with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Conditional Random Fields\n",
    "    - Lafferty et al. (2001) [Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf) (__original paper__)\n",
    "    - Sutton & McCallum's [An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf)\n",
    "    - Edwin Chen's [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "    - Michael Collin's [Log-Linear Models, MEMMs, and CRFs](http://www.cs.columbia.edu/~mcollins/crf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 8: Sequence Labeling for Parts of Speech and Named Entities](https://web.stanford.edu/~jurafsky/slp3/8.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 7: Extracting Information from Text](https://www.nltk.org/book/ch07.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)\n",
    "- [CRFsuite](http://www.chokkan.org/software/crfsuite/)\n",
    "    - [python-crfsuite](https://python-crfsuite.readthedocs.io) python binding to `CRFsuite`.\n",
    "    - [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) `python-crfsuite` wrapper providing API similar to `scikit-learn`\n",
    "    - you need to install both `python_crfsuite` and `sklearn_crfsuite` to use `sklearn_crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Sequence Labeling and Shallow Parsing\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *Los Angeles* is a *LOCATION* concept that has 2 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-LOC__\n",
    "    - suffix: __LOC-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)utside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        Alex       PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        LOC\n",
    "        Angeles    LOC\n",
    "        in         O\n",
    "        California LOC\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        Alex       U-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California U-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Interpretation\n",
    "Segmentation and Labeling data formats encode the following information:\n",
    "- in string (sentence) `\"Alex is going to Los Angeles in California\"`\n",
    "- there are 3 __entities__ (a.k.a. chunks, concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `PERSON`\n",
    "    - `LOCATION`\n",
    "    \n",
    "- entity of __type__ `PERSON`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[0:1]`\n",
    "    - has __value__: `\"Alex\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    " \n",
    "*CoNLL* format encodes __tokenization__ informations. In other words, how string `\"Alex is going to Los Angeles in California\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Wirte the IOB tags for the sentence `\"Steve Jobs established Apple Inc.\"`\n",
    "    - 'Steve Jobs' is PER\n",
    "    - 'Apple Inc' is ORG"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "B-PER I-PER      O       B-ORG I-ORG\n",
    "Steve Jobs  established  Apple Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Named Entity Recognition with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__.\n",
    "\n",
    "Addtionally, NLTK provides [Chunking](http://www.nltk.org/api/nltk.chunk.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. NLTK Pre-trained NE Chunker\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Pierre/NNP)\n",
      "  (ORGANIZATION Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n",
      "(S\n",
      "  (NE Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "for s in treebank.tagged_sents():\n",
    "    print(s)\n",
    "    print(nltk.ne_chunk(s))\n",
    "    print(nltk.ne_chunk(s, binary=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "('LOC', 'PER', 'ORG', 'MISC')\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
      "[('Melbourne', 'NP'), ('(', 'Fpa'), ('Australia', 'NP'), (')', 'Fpt'), (',', 'Fc'), ('25', 'Z'), ('may', 'NC'), ('(', 'Fpa'), ('EFE', 'NC'), (')', 'Fpt'), ('.', 'Fp')]\n",
      "(S\n",
      "  (LOC Melbourne/NP)\n",
      "  (/Fpa\n",
      "  (LOC Australia/NP)\n",
      "  )/Fpt\n",
      "  ,/Fc\n",
      "  25/Z\n",
      "  may/NC\n",
      "  (/Fpa\n",
      "  (ORG EFE/NC)\n",
      "  )/Fpt\n",
      "  ./Fp)\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "\n",
    "print(len(conll2002.tagged_sents()))\n",
    "print(conll2002._chunk_types)\n",
    "print(conll2002.sents('esp.train')[0])\n",
    "print(conll2002.tagged_sents('esp.train')[0])\n",
    "print(conll2002.chunked_sents('esp.train')[0])\n",
    "print(conll2002.iob_sents('esp.train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# training hmm on training data: exactly as above\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_ner = hmm_model.train(trn_sents)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_ner.accuracy(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### NLTK Chunk Tagger Note\n",
    "HMM uses only words as input, NLTK also povides trainable MaxEnt Chunker Tagger, which unfortunatelly requires `megam` file. Unfortunatelly, it is very convoluted to install. (http://www.umiacs.umd.edu/~hal/megam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B'), ('(', 'O'), ('Australia', 'B'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B'), (')', 'O'), ('.', 'O')]\n",
      "Accuracy: 0.4297\n"
     ]
    }
   ],
   "source": [
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents_seg[0])\n",
    "\n",
    "tst_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_seg = hmm_model.train(trn_sents_seg)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_seg.accuracy(tst_sents_seg)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.299</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.528</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.691</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.399</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.098</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.030  0.849  0.058  1084\n",
       "MISC   0.570  0.203  0.299   340\n",
       "ORG    0.795  0.396  0.528  1400\n",
       "PER    0.691  0.280  0.399   735\n",
       "total  0.055  0.491  0.098  3559"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references (note that it is testb this time)\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]\n",
    "print(refs[0])\n",
    "# getting hypotheses\n",
    "hyps = [hmm_ner.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "print(hyps[0])\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre Vinken', '61 years old', 'Nov. 29']\n",
      "[('PERSON', 'B'), ('PERSON', 'I'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('DATE', 'I'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "txt = 'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n",
    "doc = nlp(txt)\n",
    "\n",
    "print([ent.text for ent in doc.ents])\n",
    "print([(t.ent_type_, t.ent_iob_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shallow Parsing with Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CRFs](https://en.wikipedia.org/wiki/Conditional_random_field) are a type of __discriminative undirected probabilistic graphical model__. \n",
    "It is a generalization of __any__ undirected graph structure.\n",
    "In Natural Language Processing, the structure is a *sequence* of words, and conditioning is on *previous transition*. This is known as __Linear Chain CRFs__.\n",
    "\n",
    "For general graphs, the problem of exact inference in CRFs is intractable. For __Linear Chain CRFs__, however, there is an exact solution, and the used algorithm is \"analogous to the forward-backward and Viterbi algorithm for the case of Hidden Markov Models\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python CRF Tutorials\n",
    "\n",
    "Authors of the python packages provide tutorials in the form of notebooks already!\n",
    "\n",
    "__Follow the tutorials to learn the tools.__\n",
    "\n",
    "- [sklearn-crfsuite notebook](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "- [python-crfsuite notebook](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb)\n",
    "\n",
    "- `bias` is explained [here](https://github.com/scrapinghub/python-crfsuite/issues/73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a CRF baseline for our dataset that:\n",
    "- considers only word itself and the previous tag (similar to HMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n",
      "1915\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# data set\n",
    "print(len(trn_sents))\n",
    "print(len(tst_sents))\n",
    "print(trn_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy & re-define feature extraction functions from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our baseline features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "[{'bias': 1.0, 'word.lower()': 'melbourne'}, {'bias': 1.0, 'word.lower()': '('}, {'bias': 1.0, 'word.lower()': 'australia'}, {'bias': 1.0, 'word.lower()': ')'}, {'bias': 1.0, 'word.lower()': ','}, {'bias': 1.0, 'word.lower()': '25'}, {'bias': 1.0, 'word.lower()': 'may'}, {'bias': 1.0, 'word.lower()': '('}, {'bias': 1.0, 'word.lower()': 'efe'}, {'bias': 1.0, 'word.lower()': ')'}, {'bias': 1.0, 'word.lower()': '.'}]\n",
      "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print(trn_sents[0])\n",
    "print(sent2features(trn_sents[0]))\n",
    "print(sent2labels(trn_sents[0]))\n",
    "print(sent2tokens(trn_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.7 ms, sys: 6.59 ms, total: 102 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_feats = [sent2features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.33 s, sys: 6.75 ms, total: 6.33 s\n",
      "Wall time: 6.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'sao'}, {'bias': 1.0, 'word.lower()': 'paulo'}, {'bias': 1.0, 'word.lower()': '('}, {'bias': 1.0, 'word.lower()': 'brasil'}, {'bias': 1.0, 'word.lower()': ')'}, {'bias': 1.0, 'word.lower()': ','}, {'bias': 1.0, 'word.lower()': '23'}, {'bias': 1.0, 'word.lower()': 'may'}, {'bias': 1.0, 'word.lower()': '('}, {'bias': 1.0, 'word.lower()': 'efecom'}, {'bias': 1.0, 'word.lower()': ')'}, {'bias': 1.0, 'word.lower()': '.'}]\n"
     ]
    }
   ],
   "source": [
    "tst_feats = [sent2features(s) for s in tst_sents]\n",
    "print(tst_feats[0])\n",
    "pred = crf.predict(tst_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our `conll` evaluation script. (Notice that tools report token level metrics.)\n",
    "\n",
    "For that we will need to modify our prediction output a bit, to make it a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'bias': 1.0, 'word.lower()': 'sao'}, 'B-LOC'), ({'bias': 1.0, 'word.lower()': 'paulo'}, 'I-LOC'), ({'bias': 1.0, 'word.lower()': '('}, 'O'), ({'bias': 1.0, 'word.lower()': 'brasil'}, 'B-LOC'), ({'bias': 1.0, 'word.lower()': ')'}, 'O'), ({'bias': 1.0, 'word.lower()': ','}, 'O'), ({'bias': 1.0, 'word.lower()': '23'}, 'O'), ({'bias': 1.0, 'word.lower()': 'may'}, 'O'), ({'bias': 1.0, 'word.lower()': '('}, 'O'), ({'bias': 1.0, 'word.lower()': 'efecom'}, 'B-ORG'), ({'bias': 1.0, 'word.lower()': ')'}, 'O'), ({'bias': 1.0, 'word.lower()': '.'}, 'O')]\n"
     ]
    }
   ],
   "source": [
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "print(hyp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.706</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.542</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.352</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.729</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.859</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.729</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.569</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.699  0.713  0.706   985\n",
       "MISC   0.542  0.261  0.352   445\n",
       "ORG    0.729  0.442  0.551  1700\n",
       "PER    0.859  0.375  0.522  1222\n",
       "total  0.729  0.466  0.569  4352"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of CRFs lies in its ability to make use of rich feature representation. The process of extracting features from raw data is know as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering).\n",
    "\n",
    "Common features used in sequence labeling with CRFs are:\n",
    "- part-of-speech tags\n",
    "- lemmas\n",
    "- token character prefixes and suffixes (e.g. first and last 1, 2, 3 characters of a word; `word[-3:]` in tutorial is suffix of length 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SpaCy Token Features\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a convenient way to augment our feature set with common features using in Natural Language Processing. \n",
    "\n",
    "The list of provided token-level features is available [here](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Adding Features to CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify `sent2features` function to make use of spaCy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_sm in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from es_core_news_sm) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.30.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es_core_news_sm) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es_core_news_sm) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es_core_news_sm) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dhilab-mattia/.cache/pypoetry/virtualenvs/nlu-labs-dssUpVH1-py3.10/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es_core_news_sm) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get Spanish model of spacy\n",
    "!pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent, suffix=False):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_,\n",
    "            'suffix': token.suffix_,\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'melbourne', 'pos': 'PROPN', 'lemma': 'Melbourne'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '('}, {'bias': 1.0, 'word.lower()': 'australia', 'pos': 'PROPN', 'lemma': 'Australia'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')'}, {'bias': 1.0, 'word.lower()': ',', 'pos': 'PUNCT', 'lemma': ','}, {'bias': 1.0, 'word.lower()': '25', 'pos': 'NUM', 'lemma': '25'}, {'bias': 1.0, 'word.lower()': 'may', 'pos': 'NOUN', 'lemma': 'may'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '('}, {'bias': 1.0, 'word.lower()': 'efe', 'pos': 'PROPN', 'lemma': 'EFE'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')'}, {'bias': 1.0, 'word.lower()': '.', 'pos': 'PUNCT', 'lemma': '.'}]\n"
     ]
    }
   ],
   "source": [
    "print(trn_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.33 s, sys: 6.92 ms, total: 7.33 s\n",
      "Wall time: 7.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise is to experiment with a CRF model on NER task. You have to train and test a CRF with using different features on the conll2002 corpus (the same that we used here in the lab). \n",
    "\n",
    "The features that you have to experiment with are:\n",
    "- Baseline using the fetures in sent2spacy_features\n",
    "    - Train the model and print results on the test set\n",
    "- Add the \"suffix\" feature\n",
    "    - Train the model and print results on the test set\n",
    "- Add all the features used in the [tutorial on CoNLL dataset](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "    - Train the model and print results on the test set\n",
    "- Increase the feature window (number of previous and next token) to:\n",
    "    - `[-1, +1]`\n",
    "        - Train the model and print results on the test set\n",
    "    - `[-2, +2]`\n",
    "        - Train the model and print results on the test set\n",
    "\n",
    "        \n",
    "The format of the results has to be the table that we used so far, that is:\n",
    "```python\n",
    "results = evaluate(tst_sents, hyp)\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, pos, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, pos, label in sent]\n",
    "\n",
    "def sent2pos(sent):\n",
    "    return [pos for token, pos, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent, suffix=False, conll_tutorial=False, sorrounding_one=False, sorrounding_two=False):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    pos = sent2pos(sent)\n",
    "    feats = []\n",
    "    for index, (token, pos) in enumerate(zip(spacy_sent, pos)):\n",
    "        if suffix:\n",
    "            token_feats = {\n",
    "                'bias': 1.0,\n",
    "                'word.lower()': token.lower_,\n",
    "                'pos': token.pos_,\n",
    "                'lemma': token.lemma_,   \n",
    "                'suffix': token.suffix_,\n",
    "            }\n",
    "        elif conll_tutorial:\n",
    "            token_feats = {\n",
    "                'bias': 1.0,\n",
    "                'word.lower()': token.lower_,\n",
    "                'word[-3:]': str(token)[-3:],\n",
    "                'word[-2:]': str(token)[-2:],\n",
    "                'word.isupper()': token.is_upper,\n",
    "                'word.istitle()': token.is_title,\n",
    "                'word.isdigit()': token.is_digit,\n",
    "                'postag': pos,\n",
    "                'postag[:2]': pos[:2],     \n",
    "            }\n",
    "        else:\n",
    "            token_feats = {\n",
    "                'bias': 1.0,\n",
    "                'word.lower()': token.lower_,\n",
    "                'pos': token.pos_,\n",
    "                'lemma': token.lemma_,   \n",
    "            }\n",
    "        if sorrounding_one or sorrounding_two:\n",
    "            if index > 0:\n",
    "                token_feats.update({\n",
    "                    '-1:word.lower()': spacy_sent[index-1].lower_,\n",
    "                    '-1:word.istitle()': spacy_sent[index-1].is_title,\n",
    "                    '-1:word.isupper()': spacy_sent[index-1].is_upper,\n",
    "                    '-1:postag': spacy_sent[index-1].pos_,\n",
    "                    '-1:postag[:2]': spacy_sent[index-1].pos_[:2],\n",
    "                })\n",
    "            else:\n",
    "                token_feats['BOS'] = True\n",
    "            \n",
    "            if index < len(spacy_sent)-1:\n",
    "                token_feats.update({\n",
    "                    '+1:word.lower()': spacy_sent[index+1].lower_,\n",
    "                    '+1:word.istitle()': spacy_sent[index+1].is_title,\n",
    "                    '+1:word.isupper()': spacy_sent[index+1].is_upper,\n",
    "                    '+1:postag': spacy_sent[index+1].pos_,\n",
    "                    '+1:postag[:2]': spacy_sent[index+1].pos_[:2],\n",
    "                })\n",
    "            else:\n",
    "                token_feats['EOS'] = True\n",
    "\n",
    "        if sorrounding_two:\n",
    "            if index > 1:\n",
    "                token_feats.update({\n",
    "                    '-2:word.lower()': spacy_sent[index-2].lower_,\n",
    "                    '-2:word.istitle()': spacy_sent[index-2].is_title,\n",
    "                    '-2:word.isupper()': spacy_sent[index-2].is_upper,\n",
    "                    '-2:postag': spacy_sent[index-2].pos_,\n",
    "                    '-2:postag[:2]': spacy_sent[index-2].pos_[:2],\n",
    "                })\n",
    "            else:\n",
    "                token_feats['BOS'] = True\n",
    "            \n",
    "            if index < len(spacy_sent)-2:\n",
    "                token_feats.update({\n",
    "                    '+2:word.lower()': spacy_sent[index+2].lower_,\n",
    "                    '+2:word.istitle()': spacy_sent[index+2].is_title,\n",
    "                    '+2:word.isupper()': spacy_sent[index+2].is_upper,\n",
    "                    '+2:postag': spacy_sent[index+2].pos_,\n",
    "                    '+2:postag[:2]': spacy_sent[index+2].pos_[:2],\n",
    "                })\n",
    "            else:\n",
    "                token_feats['EOS'] = True\n",
    "\n",
    "\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn_crfsuite import CRF\n",
    "from conll import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "trn_sents = conll2002.iob_sents('esp.train')\n",
    "tst_sents = conll2002.iob_sents('esp.testa')\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "\n",
    "    trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "    trn_label = [sent2labels(s) for s in trn_sents]\n",
    "    tst_feats = [sent2spacy_features(s) for s in tst_sents]\n",
    "\n",
    "    try:\n",
    "        crf.fit(trn_feats, trn_label)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    pred = crf.predict(tst_feats)\n",
    "\n",
    "    hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "    results = evaluate(tst_sents, hyp)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    pd_tbl.round(decimals=3)\n",
    "    return pd_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix():\n",
    "\n",
    "    trn_feats = [sent2spacy_features(s, suffix=True) for s in trn_sents]\n",
    "    trn_label = [sent2labels(s) for s in trn_sents]\n",
    "    tst_feats = [sent2spacy_features(s, suffix=True) for s in tst_sents]\n",
    "\n",
    "    try:\n",
    "        crf.fit(trn_feats, trn_label)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    pred = crf.predict(tst_feats)\n",
    "\n",
    "    hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "    results = evaluate(tst_sents, hyp)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    pd_tbl.round(decimals=3)\n",
    "    return pd_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_tutorial():\n",
    "\n",
    "    trn_feats = [sent2spacy_features(s, conll_tutorial=True) for s in trn_sents]\n",
    "    trn_label = [sent2labels(s) for s in trn_sents]\n",
    "    tst_feats = [sent2spacy_features(s, conll_tutorial=True) for s in tst_sents]\n",
    "\n",
    "    try:\n",
    "        crf.fit(trn_feats, trn_label)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    pred = crf.predict(tst_feats)\n",
    "\n",
    "    hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "    results = evaluate(tst_sents, hyp)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    pd_tbl.round(decimals=3)\n",
    "    return pd_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorrounding_one():\n",
    "\n",
    "    trn_feats = [sent2spacy_features(s, sorrounding_one=True) for s in trn_sents]\n",
    "    trn_label = [sent2labels(s) for s in trn_sents]\n",
    "    tst_feats = [sent2spacy_features(s, sorrounding_one=True) for s in tst_sents]\n",
    "\n",
    "    try:\n",
    "        crf.fit(trn_feats, trn_label)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    pred = crf.predict(tst_feats)\n",
    "\n",
    "    hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "    results = evaluate(tst_sents, hyp)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    pd_tbl.round(decimals=3)\n",
    "    return pd_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorrounding_two():\n",
    "\n",
    "    trn_feats = [sent2spacy_features(s, sorrounding_two=True) for s in trn_sents]\n",
    "    trn_label = [sent2labels(s) for s in trn_sents]\n",
    "    tst_feats = [sent2spacy_features(s, sorrounding_two=True) for s in tst_sents]\n",
    "\n",
    "    try:\n",
    "        crf.fit(trn_feats, trn_label)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    pred = crf.predict(tst_feats)\n",
    "\n",
    "    hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]\n",
    "\n",
    "    results = evaluate(tst_sents, hyp)\n",
    "\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    pd_tbl.round(decimals=3)\n",
    "    return pd_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.723277</td>\n",
       "      <td>0.735025</td>\n",
       "      <td>0.729104</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.820062</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.724532</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.606195</td>\n",
       "      <td>0.725294</td>\n",
       "      <td>0.660418</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.682610</td>\n",
       "      <td>0.675551</td>\n",
       "      <td>0.679062</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "MISC   0.622951  0.426966  0.506667   445\n",
       "LOC    0.723277  0.735025  0.729104   985\n",
       "PER    0.820062  0.648936  0.724532  1222\n",
       "ORG    0.606195  0.725294  0.660418  1700\n",
       "total  0.682610  0.675551  0.679062  4352"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.613419</td>\n",
       "      <td>0.431461</td>\n",
       "      <td>0.506596</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.657068</td>\n",
       "      <td>0.764467</td>\n",
       "      <td>0.706710</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.809021</td>\n",
       "      <td>0.689853</td>\n",
       "      <td>0.744700</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.696716</td>\n",
       "      <td>0.686471</td>\n",
       "      <td>0.691556</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.707615</td>\n",
       "      <td>0.678998</td>\n",
       "      <td>0.693011</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "MISC   0.613419  0.431461  0.506596   445\n",
       "LOC    0.657068  0.764467  0.706710   985\n",
       "PER    0.809021  0.689853  0.744700  1222\n",
       "ORG    0.696716  0.686471  0.691556  1700\n",
       "total  0.707615  0.678998  0.693011  4352"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.469663</td>\n",
       "      <td>0.518610</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.616307</td>\n",
       "      <td>0.782741</td>\n",
       "      <td>0.689624</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.845216</td>\n",
       "      <td>0.737316</td>\n",
       "      <td>0.787587</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.771013</td>\n",
       "      <td>0.685294</td>\n",
       "      <td>0.725631</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.727143</td>\n",
       "      <td>0.699908</td>\n",
       "      <td>0.713265</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "MISC   0.578947  0.469663  0.518610   445\n",
       "LOC    0.616307  0.782741  0.689624   985\n",
       "PER    0.845216  0.737316  0.787587  1222\n",
       "ORG    0.771013  0.685294  0.725631  1700\n",
       "total  0.727143  0.699908  0.713265  4352"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_tutorial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.655602</td>\n",
       "      <td>0.802030</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.531609</td>\n",
       "      <td>0.415730</td>\n",
       "      <td>0.466583</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.761962</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.750821</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.856007</td>\n",
       "      <td>0.792962</td>\n",
       "      <td>0.823280</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.738469</td>\n",
       "      <td>0.735754</td>\n",
       "      <td>0.737109</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "LOC    0.655602  0.802030  0.721461   985\n",
       "MISC   0.531609  0.415730  0.466583   445\n",
       "ORG    0.761962  0.740000  0.750821  1700\n",
       "PER    0.856007  0.792962  0.823280  1222\n",
       "total  0.738469  0.735754  0.737109  4352"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorrounding_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.665541</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.726602</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.456180</td>\n",
       "      <td>0.504348</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.753563</td>\n",
       "      <td>0.746471</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.756956</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.739322</td>\n",
       "      <td>0.731847</td>\n",
       "      <td>0.735566</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "LOC    0.665541  0.800000  0.726602   985\n",
       "MISC   0.563889  0.456180  0.504348   445\n",
       "ORG    0.753563  0.746471  0.750000  1700\n",
       "PER    0.856481  0.756956  0.803649  1222\n",
       "total  0.739322  0.731847  0.735566  4352"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorrounding_two()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
