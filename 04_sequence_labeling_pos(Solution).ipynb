{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling: Part-of-Speech Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding:\n",
    "    - Relation between Classification and Sequence Labeling\n",
    "    - Relation between Ngram Modeling and Sequence Labeling\n",
    "    - General setting for Sequence Labeling\n",
    "    - Markov Model Tagging\n",
    "    - Universal Part-of-Speech Tags\n",
    "- Learning how to:\n",
    "    - perform POS-tagging using NLTK\n",
    "    - perform POS-tagging using spacy\n",
    "    - train and test (evaluate) POS-tagger with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "\n",
    "- SLP\n",
    "    - [Chapter 8: Part-of-Speech Tagging (HMMs)](https://web.stanford.edu/~jurafsky/slp3/8.pdf)\n",
    "- NLTK\n",
    "    - [Chapter 5: Part of Speech Tagging](https://www.nltk.org/book/ch05.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Labeling (Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Sequence Labeling and Classification\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n",
    "\n",
    "[Sequence Labeling](https://en.wikipedia.org/wiki/Sequence_labeling) is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. It is a sub-class of [structured (output) learning](https://en.wikipedia.org/wiki/Structured_prediction), since we are predicting a *sequence* object rather than a discrete or real value predicted in classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The problem can be treated as a set of independent classification tasks, one per member of the sequence;\n",
    "- **BUT!** performance is generally improved by making the optimal label for a given element dependent on the choices of nearby elements;\n",
    "\n",
    "Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and [approximate inference](https://en.wikipedia.org/wiki/Approximate_inference) and learning methods are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Sequence Labeling and Ngram Modeling\n",
    "[Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a stochastic model used to describe sequences. It is the simplest [Markov Model](https://en.wikipedia.org/wiki/Markov_model). In order to make inference tractable, a process that generated the sequence is assumed to have [Markov Property](https://en.wikipedia.org/wiki/Markov_property), i.e. future states depend only on the current state, not on the events that occurred before it. (An [ngram](https://en.wikipedia.org/wiki/N-gram) [language model](https://en.wikipedia.org/wiki/Language_model) is a $(n-1)$-order Markov Model.) \n",
    "\n",
    "In Statical Language Modeling, we are modeling *observed sequences* represented as Markov Chains. Since the states of the process are *observable*, we only need to compute __transition probabilities__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sequence Labeling, we assume that *observed sequences* (__sentences__) have been generated by a Markov Process with *unobservable* (i.e. hidden) states (__labels__), i.e. [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model) (__HMM__). \n",
    "Since the states of the process are hidden and the output is observable, each state has a probability distribution over the possible output tokens, i.e. __emission probabilities__. \n",
    "\n",
    "Using these two probability distributions (__transition__ and __emission__), in sequence labeling, we are *inferring* the sequence of state transitions, given a sequence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.\n",
    "\n",
    "Tag Sets vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Universal Part of Speech Tags\n",
    "\n",
    "Universal POS-Tag Set represents a simplified and unified set of part-of-speech tags, that was proposed for the standardization across corpora and languages. \n",
    "The number of defined tags varies from 12 ([Petrov et al/Google/NLTK](https://github.com/slavpetrov/universal-pos-tags)) to 17 ([Universal Dependencies/spaCy](https://universaldependencies.org/u/pos/index.html), in *Italics*).\n",
    "\n",
    "\n",
    "\n",
    "| Tag  | Meaning | English Examples |\n",
    "|:-----|:--------|:-----------------|\n",
    "| __Open Class__ |||\n",
    "| NOUN | noun (common and proper) | year, home, costs, time, Africa\n",
    "| VERB | verb (all tenses and modes) | is, say, told, given, playing, would\n",
    "| ADJ  | adjective           | new, good, high, special, big, local\n",
    "| ADV  | adverb              | really, already, still, early, now\n",
    "| *PROPN* | proper noun (split from NOUN) | Africa\n",
    "| *INTJ*  | interjection (split from X) | oh, ouch\n",
    "| __Closed Class__ |||\n",
    "| DET  | determiner, article | the, a, some, most, every, no, which\n",
    "| PRON | pronoun             | he, their, her, its, my, I, us\n",
    "| ADP  | adposition\t(prepositions and postpositions) | on, of, at, with, by, into, under\n",
    "| NUM  | numeral             | twenty-four, fourth, 1991, 14:24\n",
    "| PRT (*PART*) | particles or other function words | at, on, out, over per, that, up, with\n",
    "| CONJ | conjunction         | and, or, but, if, while, although\n",
    "| *AUX* | auxiliary (split from VERB) | have, is, should\n",
    "| *CCONJ*  | coordinating conjunction (splits CONJ) | or, and\n",
    "| *SCONJ*  | subordinating conjunction (splits CONJ) | if, while\n",
    "| __Other__ |||\n",
    "| .    | punctuation marks   | . , ; !\n",
    "| X    | other               | foreign words, typos, abbreviations: ersatz, esprit, dunno, gr8, univeristy\n",
    "| *SYM* | symbols (split from X) | $, :) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part-of-Speech Tagging with Spacy & NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Part-of-Speech Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en-core-web-sm\")\n",
    "\n",
    "# un-comment the lines below, if you get 'ModuleNotFoundError'\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "# let's print spaCy pipeline\n",
    "print([key for key, model in nlp.pipeline])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', '.', 'I', 'have', 'seen', 'a', 'man', 'with', 'a', 'telescope', 'in', 'Antarctica', '.']\n",
      "['UH', '.', 'PRP', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']\n",
      "['INTJ', 'PUNCT', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "text = \"Oh. I have seen a man with a telescope in Antarctica.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# tokens\n",
    "print([t.text for t in doc])\n",
    "\n",
    "# Fine grained POS-tags\n",
    "print([t.tag_ for t in doc])\n",
    "\n",
    "# Coarse POS-tags (from Universal POS Tag set)\n",
    "print([t.pos_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. Part-of-Speech Tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dhilab-mattia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', '.', 'I', 'have', 'seen', 'a', 'man', 'with', 'a', 'telescope', 'in', 'Antarctica', '.']\n",
      "[('Oh', 'UH'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('a', 'DT'), ('man', 'NN'), ('with', 'IN'), ('a', 'DT'), ('telescope', 'NN'), ('in', 'IN'), ('Antarctica', 'NNP'), ('.', '.')]\n",
      "[('Oh', 'X'), ('.', '.'), ('I', 'PRON'), ('have', 'VERB'), ('seen', 'VERB'), ('a', 'DET'), ('man', 'NOUN'), ('with', 'ADP'), ('a', 'DET'), ('telescope', 'NOUN'), ('in', 'ADP'), ('Antarctica', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Oh. I have seen a man with a telescope in Antarctica.\"\n",
    "\n",
    "# tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# POS-tagging (with WSJ Tags)\n",
    "print(nltk.pos_tag(tokens))\n",
    "\n",
    "# POS-tagging with Universal Tags\n",
    "print(nltk.pos_tag(tokens, tagset='universal'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3. Training POS-Tagger with NLTK\n",
    "\n",
    "- Manually POS-tagged corpus\n",
    "- Sequence Labeling (Tagging) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.1. Corpora for POS-Tagging\n",
    "NLTK provides several corpora, most of them are POS-tagged. We will use WSJ with universal tag set (automatically converted using internal mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/dhilab-\n",
      "[nltk_data]     mattia/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]]\n",
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# WSJ POS-Tags\n",
    "print(treebank.tagged_sents()[:1])\n",
    "\n",
    "# Universal POS-Tags\n",
    "print(treebank.tagged_sents(tagset='universal')[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.2. NLTK Taggers\n",
    "\n",
    "NLTK provides several tagging algorithms, including \n",
    "\n",
    "- rule-based taggers\n",
    "    - Regular Expression Tagger: assigns tags to tokens by comparing their word strings to a series of regular expressions.\n",
    "\n",
    "- [Pre-Trained Taggers](http://www.nltk.org/api/nltk.tag.html)\n",
    "    - HunPoS\n",
    "    - Senna\n",
    "    - Stanford Tagger\n",
    "    \n",
    "- trainable taggers\n",
    "    - `Brill Tagger`: Brill's transformational rule-based tagger assigns an initial tag sequence to a text; and then appies an ordered list of transformational rules to correct the tags of individual tokens. Learns rules from corpus.\n",
    "    - [Greedy Averaged Perceptron](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python)\n",
    "    - [TnT](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf)\n",
    "    - Hidden Markov Models\n",
    "    - Conditional Random Fields\n",
    "    - Sequential:\n",
    "        - Affix Tagger: A tagger that chooses a token's tag based on a leading or trailing substring of its word string.\n",
    "        - Ngram Tagger: A tagger that chooses a token's tag based on its word string and on the preceding _n_ word's tags.\n",
    "            - Unigram Tagger\n",
    "            - Bigram Tagger\n",
    "            - Trigram Tagger\n",
    "\n",
    "        - Classifier-based POS Tagger: A sequential tagger that uses a classifier to choose the tag for each token in a sentence.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.3. Testing a POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('discount', 'NOUN'), ('rate', 'NOUN'), ('on', 'ADP'), ('three-month', 'ADJ'), ('Treasury', 'NOUN'), ('bills', 'NOUN'), ('was', 'VERB'), ('essentially', 'ADV'), ('unchanged', 'ADJ'), ('at', 'ADP'), ('7.79', 'NUM'), ('%', 'NOUN'), (',', '.'), ('while', 'ADP'), ('the', 'DET'), ('rate', 'NOUN'), ('on', 'ADP'), ('six-month', 'ADJ'), ('bills', 'NOUN'), ('was', 'VERB'), ('slightly', 'ADV'), ('lower', 'ADJ'), ('at', 'ADP'), ('7.52', 'NUM'), ('%', 'NOUN'), ('compared', 'VERB'), ('with', 'ADP'), ('7.60', 'NUM'), ('%', 'NOUN'), ('Tuesday', 'NOUN'), ('.', '.')], [('Corporate', 'NOUN'), ('Issues', 'NOUN')], ...]\n",
      "Total: 3914; Train: 3132; Test: 782\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training & Test Splits as 80%/20%\n",
    "import math\n",
    "\n",
    "total_size = len(treebank.tagged_sents())\n",
    "train_indx = math.ceil(total_size * 0.8)\n",
    "trn_data = treebank.tagged_sents(tagset='universal')[:train_indx]\n",
    "tst_data = treebank.tagged_sents(tagset='universal')[train_indx:]\n",
    "print(tst_data)\n",
    "\n",
    "print(\"Total: {}; Train: {}; Test: {}\".format(total_size, len(trn_data), len(tst_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Rule-based POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "TAG  : [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'NOUN'), (',', '.'), ('will', 'NOUN'), ('join', 'NOUN'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'NOUN'), ('a', 'DET'), ('nonexecutive', 'NOUN'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "Accuracy: 0.5360\n"
     ]
    }
   ],
   "source": [
    "# rule-based tagging\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "# rules from NLTK adapted to Universal Tag Set & extended\n",
    "rules = [\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "    (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "    (r'.*able$', 'ADJ'),                # adjectives\n",
    "    (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "    (r'.*ly$', 'ADV'),                  # adverbs\n",
    "    (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "    (r'.*ing$', 'VERB'),                # gerunds\n",
    "    (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "    (r'[\\.,!\\?:;\\'\"]', '.'),            # punctuation (extension) \n",
    "    (r'.*', 'NOUN')                     # nouns (default)\n",
    "]\n",
    "\n",
    "re_tagger = RegexpTagger(rules)\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in treebank.sents()[:train_indx]:\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(re_tagger.tag(s)))\n",
    "    break\n",
    "    \n",
    "# evaluation\n",
    "accuracy = re_tagger.accuracy(tst_data)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "- Extend rule-set of RegexpTagger to handle close-class words (similar to punctuation & DET):\n",
    "\n",
    "    - prepositions (ADP)\n",
    "        - in, among, of, above, etc (add as many you want)\n",
    "    - particles (PRT)\n",
    "        - to, well, up, now, not (add as many you want)\n",
    "    - pronouns (PRON)\n",
    "        - I, you, he, she, it, they, we (add as many you want)\n",
    "    - conjunctions (CONJ)\n",
    "        - and, or, but, while, when, since (add as many you want)\n",
    "\n",
    "- Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "TAG  : [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'NOUN'), (',', '.'), ('will', 'NOUN'), ('join', 'NOUN'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'NOUN'), ('a', 'DET'), ('nonexecutive', 'NOUN'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "Accuracy: 0.6029\n"
     ]
    }
   ],
   "source": [
    "aug_rules = [\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "    (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "    (r'.*able$', 'ADJ'),                # adjectives\n",
    "    (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "    (r'.*ly$', 'ADV'),                  # adverbs\n",
    "    (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "    (r'.*ing$', 'VERB'),                # gerunds\n",
    "    (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "    (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "    (r'[\\.,!\\?:;\\'\"]', '.'),            # punctuation (extension) \n",
    "    (r'(In|in|Among|among|Above|above|as|As)$', 'ADP'),   # prepositions\n",
    "    (r'(to|To|well|Well|Up|up|Not|not|Now|now)$', 'PRT'),   # particles\n",
    "    (r'(I|you|You|He|he|She|she|It|it|They|they|We|we)$', 'PRON'),   # pronouns\n",
    "    (r'(and| or|But|but|while|since)$', 'CONJ'),# conjunctions\n",
    "    (r'.*', 'NOUN'),                     # nouns (default)\n",
    "]\n",
    "aug_re_tagger = RegexpTagger(aug_rules)\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in treebank.sents()[:train_indx]:\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(re_tagger.tag(s)))\n",
    "    break\n",
    "\n",
    "accuracy = aug_re_tagger.accuracy(tst_data)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.4. Training HMM POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "TAG  : [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "PATH : ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhilab-mattia/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/home/dhilab-mattia/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5135\n"
     ]
    }
   ],
   "source": [
    "# training hmm on treebank\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_model.train(trn_data)\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in treebank.sents()[:train_indx]:\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(hmm_tagger.tag(s)))\n",
    "    print(\"PATH : {}\".format(hmm_tagger.best_path(s)))\n",
    "    break\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_tagger.accuracy(tst_data)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab Exercise: Comparative Evaluation of NLTK Tagger and Spacy Tagger\n",
    "\n",
    "\n",
    "Train and evaluate NgramTagger\n",
    "- experiment with different tagger parameters\n",
    "- some of them have *cut-off*\n",
    "\n",
    "Evaluate `spacy` POS-tags on the same test set\n",
    "- create mapping from spacy to NLTK POS-tags \n",
    "    - SPACY list https://universaldependencies.org/u/pos/index.html\n",
    "    - NLTK list https://github.com/slavpetrov/universal-pos-tags\n",
    "- convert output to the required format (see format above)\n",
    "    - flatten into a list\n",
    "- evaluate using `accuracy` from `nltk.metrics` \n",
    "    - [link](https://www.nltk.org/_modules/nltk/metrics/scores.html#accuracy)\n",
    "        \n",
    "**Dataset**: treebank <br>\n",
    "**Expected output**: NLTK: Accuracy SPACY: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('discount', 'NOUN'), ('rate', 'NOUN'), ('on', 'ADP'), ('three-month', 'ADJ'), ('Treasury', 'NOUN'), ('bills', 'NOUN'), ('was', 'VERB'), ('essentially', 'ADV'), ('unchanged', 'ADJ'), ('at', 'ADP'), ('7.79', 'NUM'), ('%', 'NOUN'), (',', '.'), ('while', 'ADP'), ('the', 'DET'), ('rate', 'NOUN'), ('on', 'ADP'), ('six-month', 'ADJ'), ('bills', 'NOUN'), ('was', 'VERB'), ('slightly', 'ADV'), ('lower', 'ADJ'), ('at', 'ADP'), ('7.52', 'NUM'), ('%', 'NOUN'), ('compared', 'VERB'), ('with', 'ADP'), ('7.60', 'NUM'), ('%', 'NOUN'), ('Tuesday', 'NOUN'), ('.', '.')], [('Corporate', 'NOUN'), ('Issues', 'NOUN')], ...]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "total_size = len(treebank.tagged_sents())\n",
    "train_indx = math.ceil(total_size * 0.8)\n",
    "trn_data = treebank.tagged_sents(tagset='universal')[:train_indx]\n",
    "tst_data = treebank.tagged_sents(tagset='universal')[train_indx:]\n",
    "print(tst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# See abore for further details\n",
    "mapping_spacy_to_NLTK = {\n",
    "    \"ADJ\": \"ADJ\",\n",
    "    \"ADP\": \"ADP\",\n",
    "    \"ADV\": \"ADV\",\n",
    "    \"AUX\": \"VERB\",\n",
    "    \"CCONJ\": \"CONJ\",\n",
    "    \"DET\": \"DET\",\n",
    "    \"INTJ\": \"X\",\n",
    "    \"NOUN\": \"NOUN\",\n",
    "    \"NUM\": \"NUM\",\n",
    "    \"PART\": \"PRT\",\n",
    "    \"PRON\": \"PRON\",\n",
    "    \"PROPN\": \"NOUN\",\n",
    "    \"PUNCT\": \".\",\n",
    "    \"SCONJ\": \"CONJ\",\n",
    "    \"SYM\": \"X\",\n",
    "    \"VERB\": \"VERB\",\n",
    "    \"X\": \"X\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Accuracy: 0.8791\n",
      "Bigram Accuracy: 0.1342\n",
      "Trigram Accuracy: 0.0779\n",
      "Backoff Accuracy: 0.1342\n",
      "Cutoff Accuracy: 0.8158\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "\n",
    "# unigram tagger\n",
    "uni_tagger = NgramTagger(1, trn_data)\n",
    "uni_accuracy = uni_tagger.accuracy(tst_data)\n",
    "print(\"Unigram Accuracy: {:6.4f}\".format(uni_accuracy))\n",
    "\n",
    "# bigram tagger\n",
    "bi_tagger = NgramTagger(2, trn_data)\n",
    "bi_accuracy = bi_tagger.accuracy(tst_data)\n",
    "print(\"Bigram Accuracy: {:6.4f}\".format(bi_accuracy))\n",
    "\n",
    "# trigram tagger\n",
    "tri_tagger = NgramTagger(3, trn_data)\n",
    "tri_accuracy = tri_tagger.accuracy(tst_data)\n",
    "print(\"Trigram Accuracy: {:6.4f}\".format(tri_accuracy))\n",
    "\n",
    "# backoff tagger\n",
    "backoff_tagger = NgramTagger(3, trn_data, backoff=bi_tagger)\n",
    "backoff_accuracy = backoff_tagger.accuracy(tst_data)\n",
    "print(\"Backoff Accuracy: {:6.4f}\".format(backoff_accuracy))\n",
    "\n",
    "# cutoff tagger\n",
    "cutoff_tagger = NgramTagger(1, trn_data, cutoff=2)\n",
    "cutoff_accuracy = cutoff_tagger.accuracy(tst_data)\n",
    "print(\"Cutoff Accuracy: {:6.4f}\".format(cutoff_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T h e\n",
      "d i s c o u n t\n",
      "r a t e\n",
      "o n\n",
      "t h r e e - m o n t h\n",
      "T r e a s u r y\n",
      "b i l l s\n",
      "w a s\n",
      "e s s e n t i a l l y\n",
      "u n c h a n g e d\n",
      "a t\n",
      "7 . 7 9\n",
      "%\n",
      ",\n",
      "w h i l e\n",
      "t h e\n",
      "r a t e\n",
      "o n\n",
      "s i x - m o n t h\n",
      "b i l l s\n",
      "w a s\n",
      "s l i g h t l y\n",
      "l o w e r\n",
      "a t\n",
      "7 . 5 2\n",
      "%\n",
      "c o m p a r e d\n",
      "w i t h\n",
      "7 . 6 0\n",
      "%\n",
      "T u e s d a y\n",
      ".\n",
      "C o r p o r a t e\n",
      "I s s u e s\n",
      "I B M\n",
      "' s\n",
      "$\n",
      "7 5 0\n",
      "m i l l i o n\n",
      "* U *\n",
      "d e b e n t u r e\n",
      "o f f e r i n g\n",
      "d o m i n a t e d\n",
      "a c t i v i t y\n",
      "i n\n",
      "t h e\n",
      "c o r p o r a t e\n",
      "d e b t\n",
      "m a r k e t\n",
      ".\n",
      "M e a n w h i l e\n",
      ",\n",
      "m o s t\n",
      "i n v e s t m e n t - g r a d e\n",
      "b o n d s\n",
      "e n d e d\n",
      "u n c h a n g e d\n",
      "t o\n",
      "a s\n",
      "m u c h\n",
      "a s\n",
      "1 \\ / 8\n",
      "p o i n t\n",
      "h i g h e r\n",
      ".\n",
      "I n\n",
      "i t s\n",
      "l a t e s t\n",
      "c o m p i l a t i o n\n",
      "o f\n",
      "p e r f o r m a n c e\n",
      "s t a t i s t i c s\n",
      ",\n",
      "M o o d y\n",
      "' s\n",
      "I n v e s t o r s\n",
      "S e r v i c e\n",
      "f o u n d\n",
      "t h a t\n",
      "i n v e s t m e n t - g r a d e\n",
      "b o n d s\n",
      "p o s t e d\n",
      "a\n",
      "t o t a l\n",
      "r e t u r n\n",
      "o f\n",
      "2 . 7\n",
      "%\n",
      "i n\n",
      "O c t o b e r\n",
      "w h i l e\n",
      "j u n k\n",
      "b o n d s\n",
      "s h o w e d\n",
      "a\n",
      "n e g a t i v e\n",
      "r e t u r n\n",
      "o f\n",
      "1 . 5\n",
      "%\n",
      ".\n",
      "M o o d y\n",
      "' s\n",
      "s a i d\n",
      "0\n",
      "t h o s e\n",
      "r e t u r n s\n",
      "c o m p a r e\n",
      "w i t h\n",
      "a\n",
      "3 . 8\n",
      "%\n",
      "t o t a l\n",
      "r e t u r n\n",
      "f o r\n",
      "l o n g e r - t e r m\n",
      "T r e a s u r y\n",
      "n o t e s\n",
      "a n d\n",
      "b o n d s\n",
      ".\n",
      "T o t a l\n",
      "r e t u r n\n",
      "m e a s u r e s\n",
      "p r i c e\n",
      "c h a n g e s\n",
      "a n d\n",
      "i n t e r e s t\n",
      "i n c o m e\n",
      ".\n",
      "F o r\n",
      "t h e\n",
      "y e a r\n",
      "t o\n",
      "d a t e\n",
      ",\n",
      "M o o d y\n",
      "' s\n",
      "s a i d\n",
      "0\n",
      "t o t a l\n",
      "r e t u r n s\n",
      "w e r e\n",
      "t o p p e d\n",
      "* - 1\n",
      "b y\n",
      "t h e\n",
      "1 6 . 5\n",
      "%\n",
      "o f\n",
      "l o n g e r - t e r m\n",
      "T r e a s u r y\n",
      "i s s u e s\n",
      ",\n",
      "* - 2\n",
      "c l o s e l y\n",
      "f o l l o w e d\n",
      "* - 3\n",
      "b y\n",
      "1 5\n",
      "%\n",
      "f o r\n",
      "i n v e s t m e n t - g r a d e\n",
      "b o n d s\n",
      ".\n",
      "J u n k\n",
      "b o n d s\n",
      "t r a i l e d\n",
      "t h e\n",
      "g r o u p\n",
      "a g a i n\n",
      ".\n",
      "` `\n",
      "E v e n\n",
      "t h e\n",
      "7 . 2\n",
      "%\n",
      "r e t u r n\n",
      "f r o m\n",
      "t h e\n",
      "r i s k - f r e e\n",
      "t h r e e - m o n t h\n",
      "T r e a s u r y\n",
      "b i l l\n",
      "h a s\n",
      "e a s i l y\n",
      "o u t d i s t a n c e d\n",
      "t h e\n",
      "4 . 1\n",
      "%\n",
      "r e t u r n\n",
      "f r o m\n",
      "j u n k\n",
      "b o n d s\n",
      ",\n",
      "' '\n",
      "w r o t e\n",
      "* T * - 1\n",
      "M o o d y\n",
      "' s\n",
      "e c o n o m i s t\n",
      "J o h n\n",
      "L o n s k i\n",
      "i n\n",
      "y e s t e r d a y\n",
      "' s\n",
      "m a r k e t\n",
      "r e p o r t\n",
      ".\n",
      "` `\n",
      "L i t t l e\n",
      "w o n d e r\n",
      "t h a t\n",
      "b u y e r s\n",
      "f o r\n",
      "j u n k\n",
      "h a v e\n",
      "b e e n\n",
      "f o u n d\n",
      "* - 1\n",
      "w a n t i n g\n",
      ",\n",
      "' '\n",
      "h e\n",
      "s a i d\n",
      "* T * - 2\n",
      ".\n",
      "M o o d y\n",
      "' s\n",
      "s a i d\n",
      "0\n",
      "t h e\n",
      "a v e r a g e\n",
      "n e t\n",
      "a s s e t\n",
      "v a l u e\n",
      "o f\n",
      "2 4\n",
      "j u n k - b o n d\n",
      "m u t u a l\n",
      "f u n d s\n",
      "f e l l\n",
      "b y\n",
      "4 . 2\n",
      "%\n",
      "i n\n",
      "O c t o b e r\n",
      ".\n",
      "M o r t g a g e - B a c k e d\n",
      "I s s u e s\n",
      "M o r t g a g e\n",
      "s e c u r i t i e s\n",
      "e n d e d\n",
      "s l i g h t l y\n",
      "h i g h e r\n",
      "b u t\n",
      "t r a i l e d\n",
      "g a i n s\n",
      "i n\n",
      "t h e\n",
      "T r e a s u r y\n",
      "m a r k e t\n",
      ".\n",
      "G i n n i e\n",
      "M a e\n",
      "' s\n",
      "9\n",
      "%\n",
      "i s s u e\n",
      "f o r\n",
      "N o v e m b e r\n",
      "d e l i v e r y\n",
      "f i n i s h e d\n",
      "a t\n",
      "9 8\n",
      "5 \\ / 8\n",
      ",\n",
      "u p\n",
      "2 \\ / 3 2\n",
      ",\n",
      "a n d\n",
      "i t s\n",
      "9\n",
      "1 \\ / 2\n",
      "%\n",
      "i s s u e\n",
      "a t\n",
      "1 0 0\n",
      "2 2 \\ / 3 2\n",
      ",\n",
      "a l s o\n",
      "u p\n",
      "2 \\ / 3 2\n",
      ".\n",
      "T h e\n",
      "G i n n i e\n",
      "M a e\n",
      "9\n",
      "%\n",
      "s e c u r i t i e s\n",
      "w e r e\n",
      "y i e l d i n g\n",
      "9 . 3 2\n",
      "%\n",
      "t o\n",
      "a\n",
      "1 2 - y e a r\n",
      "a v e r a g e\n",
      "l i f e\n",
      ".\n",
      "A c t i v i t y\n",
      "w a s\n",
      "l i g h t\n",
      "i n\n",
      "d e r i v a t i v e\n",
      "m a r k e t s\n",
      ",\n",
      "w i t h\n",
      "n o\n",
      "n e w\n",
      "i s s u e s\n",
      "p r i c e d\n",
      "* - 1\n",
      ".\n",
      "M u n i c i p a l\n",
      "I s s u e s\n",
      "M u n i c i p a l\n",
      "b o n d s\n",
      "w e r e\n",
      "m o s t l y\n",
      "u n c h a n g e d\n",
      "t o\n",
      "u p\n",
      "1 \\ / 8\n",
      "p o i n t\n",
      "i n\n",
      "l i g h t\n",
      ",\n",
      "c a u t i o u s\n",
      "t r a d i n g\n",
      "p r i o r\n",
      "t o\n",
      "t o m o r r o w\n",
      "' s\n",
      "u n e m p l o y m e n t\n",
      "r e p o r t\n",
      ".\n",
      "A\n",
      "$\n",
      "1 1 4\n",
      "m i l l i o n\n",
      "* U *\n",
      "i s s u e\n",
      "o f\n",
      "h e a l t h\n",
      "f a c i l i t y\n",
      "r e v e n u e\n",
      "b o n d s\n",
      "f r o m\n",
      "t h e\n",
      "C a l i f o r n i a\n",
      "H e a l t h\n",
      "F a c i l i t i e s\n",
      "F i n a n c i n g\n",
      "A u t h o r i t y\n",
      "w a s\n",
      "t e m p o r a r i l y\n",
      "w i t h d r a w n\n",
      "* - 1\n",
      "a f t e r\n",
      "* - 1\n",
      "b e i n g\n",
      "t e n t a t i v e l y\n",
      "p r i c e d\n",
      "* - 2\n",
      "b y\n",
      "a\n",
      "F i r s t\n",
      "B o s t o n\n",
      "C o r p .\n",
      "g r o u p\n",
      ".\n",
      "A n\n",
      "o f f i c i a l\n",
      "f o r\n",
      "t h e\n",
      "l e a d\n",
      "u n d e r w r i t e r\n",
      "d e c l i n e d\n",
      "* - 1\n",
      "t o\n",
      "c o m m e n t\n",
      "o n\n",
      "t h e\n",
      "r e a s o n\n",
      "f o r\n",
      "t h e\n",
      "d e l a y\n",
      ",\n",
      "b u t\n",
      "m a r k e t\n",
      "p a r t i c i p a n t s\n",
      "s p e c u l a t e d\n",
      "t h a t\n",
      "a\n",
      "n u m b e r\n",
      "o f\n",
      "f a c t o r s\n",
      ",\n",
      "i n c l u d i n g\n",
      "a\n",
      "l a c k\n",
      "o f\n",
      "i n v e s t o r\n",
      "i n t e r e s t\n",
      ",\n",
      "w e r e\n",
      "r e s p o n s i b l e\n",
      ".\n",
      "T h e\n",
      "i s s u e\n",
      "c o u l d\n",
      "b e\n",
      "r e l a u n c h e d\n",
      "* - 1\n",
      ",\n",
      "p o s s i b l y\n",
      "i n\n",
      "a\n",
      "r e s t r u c t u r e d\n",
      "f o r m\n",
      ",\n",
      "a s\n",
      "e a r l y\n",
      "a s\n",
      "n e x t\n",
      "w e e k\n",
      ",\n",
      "a c c o r d i n g\n",
      "t o\n",
      "t h e\n",
      "l e a d\n",
      "u n d e r w r i t e r\n",
      ".\n",
      "A\n",
      "$\n",
      "1 0 7 . 0 3\n",
      "m i l l i o n\n",
      "* U *\n",
      "o f f e r i n g\n",
      "o f\n",
      "S a n t a\n",
      "A n a\n",
      "C o m m u n i t y\n",
      "R e d e v e l o p m e n t\n",
      "A g e n c y\n",
      ",\n",
      "C a l i f .\n",
      ",\n",
      "t a x\n",
      "a l l o c a t i o n\n",
      "b o n d s\n",
      "g o t\n",
      "o f f\n",
      "t o\n",
      "a\n",
      "s l o w\n",
      "s t a r t\n",
      "a n d\n",
      "m a y\n",
      "b e\n",
      "r e p r i c e d\n",
      "* - 1\n",
      "a t\n",
      "l o w e r\n",
      "l e v e l s\n",
      "t o d a y\n",
      ",\n",
      "a c c o r d i n g\n",
      "t o\n",
      "a n\n",
      "o f f i c i a l\n",
      "w i t h\n",
      "l e a d\n",
      "u n d e r w r i t e r\n",
      "D o n a l d s o n\n",
      "L u f k i n\n",
      "&\n",
      "J e n r e t t e\n",
      "S e c u r i t i e s\n",
      "C o r p\n",
      ".\n",
      "T h e\n",
      "S a n t a\n",
      "A n a\n",
      "b o n d s\n",
      "w e r e\n",
      "t e n t a t i v e l y\n",
      "p r i c e d\n",
      "* - 1\n",
      "* - 2\n",
      "t o\n",
      "y i e l d\n",
      "f r o m\n",
      "6 . 4 0\n",
      "%\n",
      "i n\n",
      "1 9 9 1\n",
      "t o\n",
      "7 . 4 5 8\n",
      "%\n",
      "i n\n",
      "* - 1\n",
      "B u c k i n g\n",
      "t h e\n",
      "m a r k e t\n",
      "t r e n d\n",
      ",\n",
      "a n\n",
      "i s s u e\n",
      "o f\n",
      "$\n",
      "1 3 0\n",
      "m i l l i o n\n",
      "* U *\n",
      "g e n e r a l\n",
      "o b l i g a t i o n\n",
      "d i s t r i b u t a b l e\n",
      "s t a t e\n",
      "a i d\n",
      "b o n d s\n",
      "f r o m\n",
      "D e t r o i t\n",
      ",\n",
      "M i c h .\n",
      ",\n",
      "a p p a r e n t l y\n",
      "d r e w\n",
      "s o l i d\n",
      "i n v e s t o r\n",
      "i n t e r e s t\n",
      ".\n",
      "T h e y\n",
      "w e r e\n",
      "t e n t a t i v e l y\n",
      "p r i c e d\n",
      "* - 1\n",
      "* - 2\n",
      "t o\n",
      "y i e l d\n",
      "f r o m\n",
      "6 . 2 0\n",
      "%\n",
      "i n\n",
      "1 9 9 1\n",
      "t o\n",
      "7 . 2 7 2\n",
      "%\n",
      "i n\n",
      "F o r e i g n\n",
      "B o n d\n",
      "W e s t\n",
      "G e r m a n\n",
      "d e a l e r s\n",
      "s a i d\n",
      "0\n",
      "t h e r e\n",
      "w a s\n",
      "l i t t l e\n",
      "i n t e r e s t\n",
      "i n\n",
      "T r e a s u r y\n",
      "b o n d s\n",
      "a h e a d\n",
      "o f\n",
      "T h u r s d a y\n",
      "' s\n",
      "n e w\n",
      "g o v e r n m e n t\n",
      "b o n d\n",
      "i s s u e\n",
      ".\n",
      "S o\n",
      "f a r\n",
      ",\n",
      "t h e y\n",
      "s a i d\n",
      "0\n",
      "* T * - 2\n",
      ",\n",
      "i n v e s t o r s\n",
      "a p p e a r\n",
      "u n e n t h u s i a s t i c\n",
      "a b o u t\n",
      "t h e\n",
      "n e w\n",
      "i s s u e\n",
      "w h i c h\n",
      "* T * - 1\n",
      "m i g h t\n",
      "f o r c e\n",
      "t h e\n",
      "g o v e r n m e n t\n",
      "t o\n",
      "r a i s e\n",
      "t h e\n",
      "c o u p o n\n",
      "t o\n",
      "m o r e\n",
      "t h a n\n",
      "7\n",
      "%\n",
      ".\n",
      "I t\n",
      "i s\n",
      "g e n e r a l l y\n",
      "e x p e c t e d\n",
      "* - 1\n",
      "t o\n",
      "b e\n",
      "t h e\n",
      "u s u a l\n",
      "1 0 - y e a r\n",
      ",\n",
      "f o u r\n",
      "b i l l i o n\n",
      "m a r k\n",
      "i s s u e\n",
      ".\n",
      "R u m o r s\n",
      "t o\n",
      "t h e\n",
      "c o n t r a r y\n",
      "h a v e\n",
      "b e e n\n",
      "t h a t\n",
      "i t\n",
      "w o u l d\n",
      "b e\n",
      "a\n",
      "s i x\n",
      "b i l l i o n\n",
      "m a r k\n",
      "i s s u e\n",
      ",\n",
      "o r\n",
      "t h a t\n",
      "t h e\n",
      "l a s t\n",
      "B u n d\n",
      ",\n",
      "a\n",
      "7\n",
      "%\n",
      "i s s u e\n",
      "d u e\n",
      "O c t o b e r\n",
      "1 9 9 9\n",
      ",\n",
      "w o u l d\n",
      "b e\n",
      "i n c r e a s e d\n",
      "* - 1\n",
      "b y\n",
      "t w o\n",
      "b i l l i o n\n",
      "m a r k s\n",
      ".\n",
      "E l s e w h e r e\n",
      ":\n",
      "- -\n",
      "I n\n",
      "J a p a n\n",
      ",\n",
      "t h e\n",
      "b e n c h m a r k\n",
      "N o .\n",
      "1 1 1\n",
      "4 . 6\n",
      "%\n",
      "i s s u e\n",
      "d u e\n",
      "1 9 9 8\n",
      "e n d e d\n",
      "o n\n",
      "b r o k e r s\n",
      "s c r e e n s\n",
      "u n c h a n g e d\n",
      "a t\n",
      "9 5 . 0 9\n",
      "* - 1\n",
      "t o\n",
      "y i e l d\n",
      "5 . 4 3 5\n",
      "%\n",
      ".\n",
      "- -\n",
      "I n\n",
      "B r i t a i n\n",
      ",\n",
      "t h e\n",
      "b e n c h m a r k\n",
      "1 1\n",
      "3 \\ / 4\n",
      "%\n",
      "b o n d\n",
      "d u e\n",
      "2 0 0 3 \\ / 2 0 0 7\n",
      "f e l l\n",
      "1 4 \\ / 3 2\n",
      "t o\n",
      "1 1 1\n",
      "2 \\ / 3 2\n",
      "* - 1\n",
      "t o\n",
      "y i e l d\n",
      "1 0 . 1 9\n",
      "%\n",
      ".\n",
      "T h e\n",
      "1 2\n",
      "%\n",
      "n o t e s\n",
      "d u e\n",
      "1 9 9 5\n",
      "f e l l\n",
      "9 \\ / 3 2\n",
      "t o\n",
      "1 0 3\n",
      "3 \\ / 8\n",
      "* - 1\n",
      "t o\n",
      "y i e l d\n",
      "1 1 . 1 0\n",
      "%\n",
      ".\n",
      "S t a n d a r d\n",
      "&\n",
      "P o o r\n",
      "' s\n",
      "C o r p .\n",
      "l o w e r e d\n",
      "t o\n",
      "d o u b l e - C\n",
      "f r o m\n",
      "t r i p l e - C\n",
      "t h e\n",
      "r a t i n g\n",
      "o n\n",
      "a b o u t\n",
      "$\n",
      "1 3 0\n",
      "m i l l i o n\n",
      "* U *\n",
      "o f\n",
      "d e b t\n",
      ".\n",
      "T h e\n",
      "r a t i n g\n",
      "c o n c e r n\n",
      "s a i d\n",
      "0\n",
      "t h e\n",
      "t e x t i l e\n",
      "a n d\n",
      "c l o t h i n g\n",
      "c o m p a n y\n",
      "' s\n",
      "i n t e r e s t\n",
      "e x p e n s e\n",
      "e x c e e d s\n",
      "o p e r a t i n g\n",
      "p r o f i t\n",
      "` `\n",
      "b y\n",
      "a\n",
      "w i d e\n",
      "m a r g i n\n",
      "' '\n",
      "a n d\n",
      "i t\n",
      "n o t e d\n",
      "U n i t e d\n",
      "' s\n",
      "e s t i m a t e d\n",
      "a f t e r - t a x\n",
      "l o s s\n",
      "o f\n",
      "$\n",
      "2 4\n",
      "m i l l i o n\n",
      "* U *\n",
      "f o r\n",
      "t h e\n",
      "y e a r\n",
      "e n d e d\n",
      "J u n e\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m word, _ \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(tst_data)):\n\u001b[0;32m---> 32\u001b[0m     doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(word))\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(doc)\n\u001b[1;32m     34\u001b[0m     data\u001b[39m.\u001b[39mappend([(x\u001b[39m.\u001b[39mtext, mapping_spacy_to_NLTK[x\u001b[39m.\u001b[39mpos_]) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m doc])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[1;32m     34\u001b[0m         X,\n\u001b[1;32m     35\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[1;32m     36\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     37\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[1;32m     38\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/with_array.py:32\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     29\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[1;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/with_array.py:87\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[1;32m     84\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     86\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 87\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[1;32m     89\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[1;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/layers/hashembed.py:63\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, ids, is_train)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     61\u001b[0m     model: Model[Ints1d, OutT], ids: Ints1d, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     62\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 63\u001b[0m     vectors \u001b[39m=\u001b[39m cast(Floats2d, model\u001b[39m.\u001b[39;49mget_param(\u001b[39m\"\u001b[39;49m\u001b[39mE\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     64\u001b[0m     nV \u001b[39m=\u001b[39m vectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m     nO \u001b[39m=\u001b[39m vectors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/thinc/model.py:213\u001b[0m, in \u001b[0;36mModel.get_param\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_param\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FloatsXd:\n\u001b[1;32m    212\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Retrieve a weights parameter by name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_params:\n\u001b[1;32m    214\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown param: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params\u001b[39m.\u001b[39mhas_param(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid, name):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from nltk.metrics import accuracy\n",
    "# from itertools import chain\n",
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# nlp = en_core_web_sm.load()\n",
    "# # We overwrite the spacy tokenizer with a custom one, that split by whitespace only\n",
    "# nlp.tokenizer = Tokenizer(nlp.vocab) # Tokenize by whitespace\n",
    "\n",
    "# # We create a list of tuples (word, tag) for each sentence\n",
    "# # We use the mapping_spacy_to_NLTK to map the spacy tags to the NLTK ones\n",
    "# data = []\n",
    "# for (word, _) in chain()\n",
    "#     print(word)\n",
    "#     doc = nlp(\" \".join(word))\n",
    "#     data.append([(x.text, mapping_spacy_to_NLTK[x.pos_]) for x in doc])\n",
    "\n",
    "# # compute the accuracy of converting spacy tags to NLTK tags\n",
    "# acc = accuracy(list(chain(*tst_data)), list(chain(*data)))\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from itertools import chain\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = en_core_web_sm.load()\n",
    "# We overwrite the spacy tokenizer with a custom one, that split by whitespace only\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab) # Tokenize by whitespace\n",
    "\n",
    "data = []\n",
    "for word, _ in list(chain.from_iterable(tst_data)):\n",
    "    # convert tag to spacy tag\n",
    "    pass\n",
    "\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"don't\", 'do', 'that.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.text for x in nlp(\"we don't do that.\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
